{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 推荐模型\n",
    "# 协同过滤：矩阵分解、自编码器\n",
    "# 内容过滤：因子分解机\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2526 9555 (2533,) (9565,)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from scipy.sparse import spmatrix\n",
    "\n",
    "# 加载 数据集 movie_ratings.csv\n",
    "movie_ratings = pd.read_csv('./data/movie_ratings.csv', header=None)\n",
    "movie_ratings.columns = ['user_id', 'item_id', 'rating']\n",
    "user_ids = movie_ratings.value_counts(subset=['user_id'])\n",
    "item_ids = movie_ratings.value_counts(subset=['item_id'])\n",
    "movie_ratings['rating'] = (movie_ratings['rating'] - 0) / (movie_ratings['rating'].max() - 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(movie_ratings.values[:,:2], movie_ratings.values[:,2], test_size=0.4, random_state=0)\n",
    "print(len(user_ids), len(item_ids), user_ids.keys().max(), item_ids.keys().max())\n",
    "movie_ratings.head()\n",
    "num_users = user_ids.keys().max()[0] + 1 \n",
    "num_items = item_ids.keys().max()[0] + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:28:26.725770200Z",
     "start_time": "2023-09-01T03:28:21.622711600Z"
    }
   },
   "id": "b7bfe1118340f831"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 加载数据集2\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 加载数据\n",
    "ratings = np.array([[int(x) for x in line.strip().split('\\t')[:3]] for line in open('./data/ml-100k/ua.base','r').read().strip().split('\\n')], dtype=np.int32)\n",
    "ratings[:,-1] = (ratings[:,-1] - 0)/(max(ratings[:,-1]) - 0)\n",
    "occupation_dict = {'administrator':0, 'artist':1, 'doctor':2, 'educator':3, 'engineer':4, 'entertainment':5, 'executive':6, 'healthcare':7, 'homemaker':8, 'lawyer':9, 'librarian':10, 'marketing':11, 'none':12, 'other':13, 'programmer':14, 'retired':15, 'salesman':16, 'scientist':17, 'student':18, 'technician':19, 'writer':20}\n",
    "gender_dict={'M':1,'F':0}\n",
    "user_info = {}\n",
    "for line in open('./data/ml-100k/u.user','r', encoding='utf-8').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    user_info[int(phs[0])] = [int(phs[1]), gender_dict[phs[2]], occupation_dict[phs[3]]]\n",
    "item_info = {}\n",
    "for line in open('./data/ml-100k/u.item','r', encoding='ISO-8859-1').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    item_info[int(phs[0])] = phs[5:]\n",
    "data = np.array([user_info[u] + item_info[i] + [r] for u, i, r in ratings], dtype=np.float32)\n",
    "data[:,:-1] = MinMaxScaler().fit_transform(data[:,:-1])\n",
    "# print(len(user_info[list(user_info.keys())[0]]), len(item_info[list(item_info.keys())[0]]))\n",
    "# print(data.shape)\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "num_features = 22"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T01:28:05.460110100Z",
     "start_time": "2023-09-01T01:28:01.749225400Z"
    }
   },
   "id": "dfb3f41e47bc2cf2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 21:40:09] epoch=[1/10], train_mse_loss: 0.0918, validate_mse_loss: 0.0343\n",
      "[2023-08-31 21:40:56] epoch=[2/10], train_mse_loss: 0.0347, validate_mse_loss: 0.0336\n",
      "[2023-08-31 21:41:43] epoch=[3/10], train_mse_loss: 0.0322, validate_mse_loss: 0.0322\n",
      "[2023-08-31 21:42:31] epoch=[4/10], train_mse_loss: 0.0314, validate_mse_loss: 0.0314\n",
      "[2023-08-31 21:43:20] epoch=[5/10], train_mse_loss: 0.0309, validate_mse_loss: 0.0310\n",
      "[2023-08-31 21:44:08] epoch=[6/10], train_mse_loss: 0.0306, validate_mse_loss: 0.0310\n",
      "[2023-08-31 21:44:55] epoch=[7/10], train_mse_loss: 0.0305, validate_mse_loss: 0.0309\n",
      "[2023-08-31 21:45:43] epoch=[8/10], train_mse_loss: 0.0305, validate_mse_loss: 0.0308\n",
      "[2023-08-31 21:46:31] epoch=[9/10], train_mse_loss: 0.0304, validate_mse_loss: 0.0309\n",
      "[2023-08-31 21:47:19] epoch=[10/10], train_mse_loss: 0.0305, validate_mse_loss: 0.0309\n"
     ]
    }
   ],
   "source": [
    "# CF评分预测\n",
    "# 1.1 mf矩阵分解模型: R = P Q.T\n",
    "# 数据集：movie_ratings.csv\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, Parameter, MSELoss, Embedding\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=99\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).long(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).long(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 矩阵分解\n",
    "class MatrixFactorization(Module):\n",
    "    def __init__(self, num_users, num_items, dim):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.num_users, self.num_items, self.dim = num_users, num_items, dim\n",
    "        self.user_embeddings = Embedding(num_users, dim)\n",
    "        self.item_embeddings = Embedding(num_items, dim)\n",
    "    def forward(self, user_item_ids):\n",
    "        user_embeddings = self.user_embeddings(user_item_ids[:,0])\n",
    "        item_embeddings = self.item_embeddings(user_item_ids[:,1])\n",
    "        result = torch.sigmoid(torch.sum(user_embeddings * item_embeddings, dim=-1))\n",
    "        return result.squeeze()\n",
    "model = MatrixFactorization(num_users=num_users, num_items=num_items, dim=dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=2e-4)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T13:47:19.482846Z",
     "start_time": "2023-08-31T13:39:20.110027Z"
    }
   },
   "id": "17e31ee1f00442ae"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 11:30:42] epoch=[1/10], train_mse_loss: 0.0602, validate_mse_loss: 0.0276\n",
      "[2023-09-01 11:31:45] epoch=[2/10], train_mse_loss: 0.0284, validate_mse_loss: 0.0290\n",
      "[2023-09-01 11:32:47] epoch=[3/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0289\n",
      "[2023-09-01 11:33:50] epoch=[4/10], train_mse_loss: 0.0286, validate_mse_loss: 0.0289\n",
      "[2023-09-01 11:34:53] epoch=[5/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0290\n",
      "[2023-09-01 11:35:54] epoch=[6/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0288\n",
      "[2023-09-01 11:36:57] epoch=[7/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0288\n",
      "[2023-09-01 11:37:59] epoch=[8/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0291\n",
      "[2023-09-01 11:39:00] epoch=[9/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0289\n",
      "[2023-09-01 11:40:01] epoch=[10/10], train_mse_loss: 0.0286, validate_mse_loss: 0.0290\n"
     ]
    }
   ],
   "source": [
    "# CF评分预测\n",
    "# 1.2 SVD矩阵分解模型: R = P Q.T + user_bias + item_bias + bias\n",
    "# 数据集：movie_ratings.csv\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, Parameter, MSELoss, Embedding\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=99\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).long(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).long(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 矩阵分解\n",
    "class SVD(Module):\n",
    "    def __init__(self, num_users, num_items, dim):\n",
    "        super(SVD, self).__init__()\n",
    "        self.num_users, self.num_items, self.dim = num_users, num_items, dim\n",
    "        self.user_embeddings = Embedding(num_users, dim)\n",
    "        self.item_embeddings = Embedding(num_items, dim)\n",
    "        self.bias = Parameter(torch.randn((1,1)), requires_grad=True)\n",
    "        self.user_bias = Parameter(torch.randn((num_users,1)), requires_grad=True)\n",
    "        self.item_bias = Parameter(torch.randn((num_items,1)), requires_grad=True)\n",
    "    def forward(self, user_item_ids):\n",
    "        user_embeddings = self.user_embeddings(user_item_ids[:,0])\n",
    "        item_embeddings = self.item_embeddings(user_item_ids[:,1])\n",
    "        result = torch.sigmoid(torch.sum(user_embeddings * item_embeddings, dim=-1, keepdim=True) + self.user_bias[user_item_ids[:,0]] + self.item_bias[user_item_ids[:,1]] + self.bias)\n",
    "        return result.squeeze()\n",
    "model = SVD(num_users=num_users, num_items=num_items, dim=dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=2e-4)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:40:01.319976900Z",
     "start_time": "2023-09-01T03:29:40.222561500Z"
    }
   },
   "id": "c011420406aa913b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 22:41:17] epoch=[1/10], train_mse_loss: 0.0380, validate_mse_loss: 0.0300\n",
      "[2023-08-31 22:41:18] epoch=[2/10], train_mse_loss: 0.0287, validate_mse_loss: 0.0286\n",
      "[2023-08-31 22:41:18] epoch=[3/10], train_mse_loss: 0.0275, validate_mse_loss: 0.0283\n",
      "[2023-08-31 22:41:19] epoch=[4/10], train_mse_loss: 0.0261, validate_mse_loss: 0.0282\n",
      "[2023-08-31 22:41:19] epoch=[5/10], train_mse_loss: 0.0245, validate_mse_loss: 0.0280\n",
      "[2023-08-31 22:41:20] epoch=[6/10], train_mse_loss: 0.0228, validate_mse_loss: 0.0282\n",
      "[2023-08-31 22:41:21] epoch=[7/10], train_mse_loss: 0.0213, validate_mse_loss: 0.0284\n",
      "[2023-08-31 22:41:21] epoch=[8/10], train_mse_loss: 0.0198, validate_mse_loss: 0.0289\n",
      "[2023-08-31 22:41:22] epoch=[9/10], train_mse_loss: 0.0185, validate_mse_loss: 0.0290\n",
      "[2023-08-31 22:41:22] epoch=[10/10], train_mse_loss: 0.0172, validate_mse_loss: 0.0290\n"
     ]
    }
   ],
   "source": [
    "# CF评分预测\n",
    "# 2.自编码器: R -> z -> R_\n",
    "# 数据集：movie_ratings.csv\n",
    "\n",
    "import torch, numpy as np\n",
    "from torch.nn import Module, Parameter, MSELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=99\n",
    "# train：\n",
    "train = np.zeros((num_users, num_items), dtype=np.float32)\n",
    "for f, l in zip(X_train, y_train):\n",
    "    train[int(f[0]), int(f[1])] = l\n",
    "# with mask\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(train).float(), torch.from_numpy(train>0.01).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# test:\n",
    "test = np.zeros((num_users, num_items), dtype=np.float32)\n",
    "for f, l in zip(X_test, y_test):\n",
    "    test[int(f[0]), int(f[1])] = l\n",
    "# with mask\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(test).float(), torch.from_numpy(test>0.01).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "# 自编码器\n",
    "class AutoEncoder(Module):\n",
    "    def __init__(self, num_users, num_items, dim):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.dim = dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(num_items, dim), nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(nn.Linear(dim, num_items))\n",
    "    def forward(self, torch_input):\n",
    "        encoder = self.encoder(torch_input)\n",
    "        decoder = self.decoder(encoder)\n",
    "        return torch.sigmoid(decoder).squeeze()\n",
    "model = AutoEncoder(num_users=num_users, num_items=num_items, dim=dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=2e-4)\n",
    "criterion = MSELoss(reduction='none').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        mask = inputs[1].to(device)\n",
    "        if mask.sum().item()==0:\n",
    "            continue\n",
    "        output = model(input)\n",
    "        loss = torch.sum(criterion(output, input) * mask)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([mask.sum().item(), loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        mask = inputs[1].to(device)\n",
    "        if mask.sum().item()==0:\n",
    "            continue\n",
    "        output = model(input)\n",
    "        loss = torch.sum(criterion(output, input) * mask)\n",
    "        epoch_test_losses.append([mask.sum().item(), loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T14:41:22.825921Z",
     "start_time": "2023-08-31T14:41:15.448138Z"
    }
   },
   "id": "e9fb83f10079f209"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 09:33:29] epoch=[1/10], train_mse_loss: 0.1662, validate_mse_loss: 0.1674\n",
      "[2023-09-01 09:33:32] epoch=[2/10], train_mse_loss: 0.1659, validate_mse_loss: 0.1670\n",
      "[2023-09-01 09:33:36] epoch=[3/10], train_mse_loss: 0.1660, validate_mse_loss: 0.1666\n",
      "[2023-09-01 09:33:43] epoch=[4/10], train_mse_loss: 0.1659, validate_mse_loss: 0.1668\n",
      "[2023-09-01 09:33:50] epoch=[5/10], train_mse_loss: 0.1658, validate_mse_loss: 0.1667\n",
      "[2023-09-01 09:33:58] epoch=[6/10], train_mse_loss: 0.1658, validate_mse_loss: 0.1671\n",
      "[2023-09-01 09:34:05] epoch=[7/10], train_mse_loss: 0.1658, validate_mse_loss: 0.1666\n",
      "[2023-09-01 09:34:12] epoch=[8/10], train_mse_loss: 0.1657, validate_mse_loss: 0.1667\n",
      "[2023-09-01 09:34:20] epoch=[9/10], train_mse_loss: 0.1656, validate_mse_loss: 0.1666\n",
      "[2023-09-01 09:34:27] epoch=[10/10], train_mse_loss: 0.1656, validate_mse_loss: 0.1666\n"
     ]
    }
   ],
   "source": [
    "# CF评分预测\n",
    "# 3.神经协同过滤 NeuralCF: MF + MLP 独立\n",
    "# NeuralCF的实现，用于召回\n",
    "# WWW 2017] Neural Collaborative Filtering\n",
    "# 数据集：movie_ratings.csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim = 20\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).long(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).long(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class NeuralMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim, mlp_dim, dnn_layer_dims):\n",
    "        super(NeuralMF, self).__init__()\n",
    "        self.MF_Embedding_User = nn.Embedding(num_embeddings=num_users, embedding_dim=mf_dim)\n",
    "        self.MF_Embedding_Item = nn.Embedding(num_embeddings=num_items, embedding_dim=mf_dim)\n",
    "        self.MLP_Embedding_User = nn.Embedding(num_embeddings=num_users, embedding_dim=mlp_dim)\n",
    "        self.MLP_Embedding_Item = nn.Embedding(num_embeddings=num_items, embedding_dim=mlp_dim)\n",
    "        # 全连接网络\n",
    "        self.dnn_network = nn.Sequential(nn.Linear(2 * mlp_dim, dnn_layer_dims[0]))\n",
    "        if len(dnn_layer_dims)>1:\n",
    "            for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "                self.dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "                self.dnn_network.append(nn.ReLU())\n",
    "        self.dnn_network.append(nn.Linear(dnn_layer_dims[-1], mf_dim))\n",
    "        # 合并\n",
    "        self.final_nn = nn.Sequential(nn.Linear(2 * mf_dim, 1), nn.Sigmoid())\n",
    "    def forward(self, inputs):\n",
    "        # mf\n",
    "        mf_vec = self.MF_Embedding_User(inputs[:, 0]) * self.MF_Embedding_Item(inputs[:, 1])\n",
    "        # mlp\n",
    "        mlp_vec = torch.cat([self.MLP_Embedding_User(inputs[:, 0]), self.MLP_Embedding_Item(inputs[:, 1])], dim=-1)\n",
    "        mlp_vec = self.dnn_network(mlp_vec)\n",
    "        # 合并两个\n",
    "        result = self.final_nn(torch.cat([mf_vec, mlp_vec], dim=-1))\n",
    "        return result.squeeze()\n",
    "model = NeuralMF(num_users=num_users, num_items=num_items, mf_dim=dim, mlp_dim=dim, dnn_layer_dims=[30]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=2e-4)\n",
    "criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T01:34:27.412299600Z",
     "start_time": "2023-09-01T01:33:26.680881800Z"
    }
   },
   "id": "f8994228ae5117d7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 09:29:47] epoch=[1/10], train_mse_loss: 0.2159, validate_mse_loss: 0.2086\n",
      "[2023-09-01 09:29:49] epoch=[2/10], train_mse_loss: 0.2061, validate_mse_loss: 0.2053\n",
      "[2023-09-01 09:29:50] epoch=[3/10], train_mse_loss: 0.1970, validate_mse_loss: 0.1976\n",
      "[2023-09-01 09:29:52] epoch=[4/10], train_mse_loss: 0.1865, validate_mse_loss: 0.1806\n",
      "[2023-09-01 09:29:54] epoch=[5/10], train_mse_loss: 0.1746, validate_mse_loss: 0.1720\n",
      "[2023-09-01 09:29:56] epoch=[6/10], train_mse_loss: 0.1675, validate_mse_loss: 0.1660\n",
      "[2023-09-01 09:29:57] epoch=[7/10], train_mse_loss: 0.1644, validate_mse_loss: 0.1670\n",
      "[2023-09-01 09:29:59] epoch=[8/10], train_mse_loss: 0.1627, validate_mse_loss: 0.1650\n",
      "[2023-09-01 09:30:01] epoch=[9/10], train_mse_loss: 0.1619, validate_mse_loss: 0.1622\n",
      "[2023-09-01 09:30:03] epoch=[10/10], train_mse_loss: 0.1611, validate_mse_loss: 0.1631\n"
     ]
    }
   ],
   "source": [
    "# 内容过滤：\n",
    "# FM 分解机：y = w0 + sum(w1 * f) + sum(<v_i,v_j> * f_i f_j)\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Parameter, MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=10\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.4, random_state=0)\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# bit-wise cross\n",
    "class FactorizationMachine(Module):\n",
    "    def __init__(self, num_features, dim):\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.dim = dim\n",
    "        self.w0 = Parameter(torch.randn((1,1)), requires_grad=True)\n",
    "        self.w = Parameter(torch.randn((num_features, 1)), requires_grad=True)\n",
    "        self.bw = Parameter(torch.randn((num_features, dim)), requires_grad=True)\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        # 实现方式一：\n",
    "        # tmp = torch.matmul(self.bw, self.bw.T)\n",
    "        # tmp[np.tril_indices(self.num_features)] = 0\n",
    "        # y = torch.sigmoid(self.w0 + torch.sum(torch.matmul(X, self.w), dim=-1, keepdim=True) +  torch.sum(torch.sum(torch.bmm(X.unsqueeze(-1), X.unsqueeze(1)) * tmp.unsqueeze(0), dim=-1, keepdim=False), dim=-1, keepdim=True))\n",
    "        # 实现方式二：和的平方 - 平方的和\n",
    "        sum_square = torch.sum(self.bw.unsqueeze(0) * X.unsqueeze(-1).repeat(1,1,self.dim), dim=1).square()\n",
    "        square_sum = torch.sum(self.bw.square().unsqueeze(0) * X.square().unsqueeze(-1).repeat(1,1,self.dim), dim=1)\n",
    "        y = torch.sigmoid(self.w0 + torch.sum(torch.matmul(X, self.w), dim=-1, keepdim=True) +  torch.sum(sum_square - square_sum, dim=-1, keepdim=True))\n",
    "        return y.squeeze()\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [self.w0, self.w, self.bw]\n",
    "model = FactorizationMachine(num_features=num_features, dim=dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=2e-4)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T01:30:03.330966900Z",
     "start_time": "2023-09-01T01:29:45.910451Z"
    }
   },
   "id": "f70af7981380aad6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ee312aa1a8fa7be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
