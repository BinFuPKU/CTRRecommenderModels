{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 序列推荐模型\n",
    "# GRU4Rec、DIN、DIEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dba20d208aba82",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T03:25:52.781496Z",
     "start_time": "2023-09-03T03:25:52.145421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 1548\n",
      "(446, 81)\n",
      "[[ 397  303  260  306  312  744  257  285  338  270  682  862  328 1543\n",
      "   344  881  326  298  867  265  673 1491  301  337  353  261  300 1260\n",
      "  1238  302  325  334  331  351  347 1090  683  901  897  272  345   49\n",
      "   585  309  521  126  386 1282 1038  539  288  417  418  931 1444  804\n",
      "   164  933  941 1326  686 1001  316  324 1128  900 1091 1349  710  716\n",
      "   470 1499  225   98 1508  357  365  757  887  248  633]\n",
      " [ 344  773  365  397  445   48  374   62  431  981  231  780  384  109\n",
      "    39  775 1021  929 1022  393   93  399   89  386  569  715   66 1059\n",
      "   748  414  459  418  139  832  495  831 1413  413  398  396   77  784\n",
      "   717  786   50  142  768  259 1159  394   11  917  793 1306 1307  810\n",
      "   302  944 1328  564   53  293  141  212  778  645  825  839  328  332\n",
      "  1234 1363  342 1247  485  107 1134 1137  369 1277  254]]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集2成序列数据集，评分[0,1,2]为负反馈，评分[3,4,5]为正反馈，只保留正样本，构造简单序列推荐数据集\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "random.seed(100)\n",
    "\n",
    "# 加载数据: >=3分为正，用户评分次数不低于50，只保留最后50个，拆分为40: 5 + 15负例 (随机采样): 5 + 15负例 (随机采样)\n",
    "ratings = np.array([[int(x) for x in line.strip().split('\\t')[:4]] for line in open('./data/ml-100k/ua.base','r').read().strip().split('\\n')], dtype=np.int32)\n",
    "ratings_pd = pd.DataFrame({feature_name: list(feature_data) for feature_name, feature_data in zip(['user_id','item_id','rating','timestamp'], ratings.T)})\n",
    "pos_ratings_pd = ratings_pd[ratings_pd['rating']>2.9][['user_id','item_id','timestamp']].dropna().sort_values('timestamp') # 已经排序了\n",
    "pos_ratings_pd = pos_ratings_pd.groupby('user_id').filter(lambda x: x['user_id'].count()>=50)\n",
    "userid2id = {user_id: i for i, user_id in enumerate(sorted(list(set(pos_ratings_pd['user_id'].tolist()))))}\n",
    "itemid2id = {item_id: i for i, item_id in enumerate(sorted(list(set(pos_ratings_pd['item_id'].tolist()))))}\n",
    "print(len(userid2id), len(itemid2id))\n",
    "del ratings, ratings_pd\n",
    "\n",
    "# new id\n",
    "user_train_validate_test = {}\n",
    "for user,item,t in pos_ratings_pd.values:\n",
    "    u, i = userid2id[user], itemid2id[item]\n",
    "    if u not in user_train_validate_test:\n",
    "        user_train_validate_test[u] = [i]\n",
    "    else:\n",
    "        user_train_validate_test[u].append(i)\n",
    "    user_train_validate_test[u] = user_train_validate_test[u][-50:]\n",
    "train_seq_len = 40\n",
    "pos_num = 5\n",
    "neg_sample_num = 15\n",
    "def sample(low, high, notinset, num):\n",
    "    nums = set([])\n",
    "    n = num\n",
    "    while n>0:\n",
    "        id = random.randint(low, high)\n",
    "        if id not in notinset and id not in nums:\n",
    "            nums.add(id)\n",
    "            n -= 1\n",
    "    return list(nums)\n",
    "data = np.zeros((len(user_train_validate_test), 81), dtype=np.int32)\n",
    "i = 0\n",
    "for user, train_validate_test in user_train_validate_test.items():\n",
    "    train, validate, test = train_validate_test[:train_seq_len], train_validate_test[-pos_num*2:-pos_num], train_validate_test[-pos_num:]\n",
    "    data[i, 0] = user\n",
    "    data[i,1:train_seq_len+1] = np.array(train)\n",
    "    samples = sample(0, len(itemid2id)-1, set(train_validate_test), neg_sample_num * 2)\n",
    "    data[i,1+train_seq_len : 1+train_seq_len+pos_num+neg_sample_num] = np.array(validate + samples[:neg_sample_num])\n",
    "    data[i,1+train_seq_len+pos_num+neg_sample_num : ] = np.array(test + samples[neg_sample_num:])\n",
    "    i += 1\n",
    "del user_train_validate_test\n",
    "print(data.shape)\n",
    "print(data[:2,:])\n",
    "\n",
    "# 继续加载info特征信息，内容特征\n",
    "occupation_dict = {'administrator':0, 'artist':1, 'doctor':2, 'educator':3, 'engineer':4, 'entertainment':5, 'executive':6, 'healthcare':7, 'homemaker':8, 'lawyer':9, 'librarian':10, 'marketing':11, 'none':12, 'other':13, 'programmer':14, 'retired':15, 'salesman':16, 'scientist':17, 'student':18, 'technician':19, 'writer':20}\n",
    "gender_dict={'M':1,'F':0}\n",
    "user_info = {}\n",
    "for line in open('./data/ml-100k/u.user','r', encoding='utf-8').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    if int(phs[0]) not in userid2id:\n",
    "        continue\n",
    "    uid = userid2id[int(phs[0])]\n",
    "    user_info[uid] = [gender_dict[phs[2]], occupation_dict[phs[3]]] # int(phs[1]) 为了方便，不要连续型特征\n",
    "user_num_features = 2\n",
    "item_info = {}\n",
    "for line in open('./data/ml-100k/u.item','r', encoding='ISO-8859-1').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    if int(phs[0]) not in itemid2id:\n",
    "        continue\n",
    "    iid = itemid2id[int(phs[0])]\n",
    "    item_info[iid] = phs[5:]\n",
    "item_num_features = 19\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "num_features = 21\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "133c7cc9f9ed46e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T08:22:48.694168200Z",
     "start_time": "2023-09-01T08:22:13.104247100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 16:22:16] epoch=[1/10], train_ce_loss: 0.7405, train_ndcg: 0.6901, validate_ce_loss: 0.7556, validate_ndcg: 0.6248\n",
      "[2023-09-01 16:22:20] epoch=[2/10], train_ce_loss: 0.6774, train_ndcg: 0.8633, validate_ce_loss: 0.7632, validate_ndcg: 0.6208\n",
      "[2023-09-01 16:22:23] epoch=[3/10], train_ce_loss: 0.6616, train_ndcg: 0.8945, validate_ce_loss: 0.7653, validate_ndcg: 0.6203\n",
      "[2023-09-01 16:22:27] epoch=[4/10], train_ce_loss: 0.6592, train_ndcg: 0.9063, validate_ce_loss: 0.7659, validate_ndcg: 0.6252\n",
      "[2023-09-01 16:22:30] epoch=[5/10], train_ce_loss: 0.6581, train_ndcg: 0.9107, validate_ce_loss: 0.7662, validate_ndcg: 0.6275\n",
      "[2023-09-01 16:22:34] epoch=[6/10], train_ce_loss: 0.6568, train_ndcg: 0.9145, validate_ce_loss: 0.7664, validate_ndcg: 0.6273\n",
      "[2023-09-01 16:22:38] epoch=[7/10], train_ce_loss: 0.6549, train_ndcg: 0.9156, validate_ce_loss: 0.7665, validate_ndcg: 0.6279\n",
      "[2023-09-01 16:22:41] epoch=[8/10], train_ce_loss: 0.6524, train_ndcg: 0.9182, validate_ce_loss: 0.7673, validate_ndcg: 0.6283\n",
      "[2023-09-01 16:22:45] epoch=[9/10], train_ce_loss: 0.6491, train_ndcg: 0.9213, validate_ce_loss: 0.7681, validate_ndcg: 0.6258\n",
      "[2023-09-01 16:22:48] epoch=[10/10], train_ce_loss: 0.6453, train_ndcg: 0.9226, validate_ce_loss: 0.7690, validate_ndcg: 0.6240\n"
     ]
    }
   ],
   "source": [
    "# GRU4Rec: 只用行为特征\n",
    "# user_embedding = GRU(item_embedding_seq)\n",
    "# y = user_embedding * item_embedding.T\n",
    "# 数据集：ml-100k\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, CrossEntropyLoss, Sequential, Linear, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=100\n",
    "\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(data[:,1: 1+ train_seq_len]).long(), torch.from_numpy(data[:,1+ train_seq_len:-(pos_num+neg_sample_num)]).long()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(data[:,1: 1+ train_seq_len + pos_num]).long(), torch.from_numpy(data[:,-(pos_num+neg_sample_num) : ]).long()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, gru_num_layers=1):\n",
    "        super(GRU4Rec, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim, self.gru_num_layers = embedding_dim, gru_num_layers\n",
    "        self.item_embeddings = nn.Embedding(num_items, self.embedding_dim, padding_idx=-1)\n",
    "        torch.nn.init.kaiming_normal_(self.item_embeddings.weight.data)\n",
    "        self.gru = nn.GRU(input_size=self.embedding_dim, hidden_size=self.embedding_dim, num_layers=self.gru_num_layers, batch_first=True)\n",
    "    # [batch, seq_len], [batch, label_len]\n",
    "    def forward(self, item_seqs: torch.Tensor, test: torch.Tensor):\n",
    "        batch_len = item_seqs.shape[0]\n",
    "        # [batch, seq_len, dim]\n",
    "        item_seqs_embeddings = self.item_embeddings(item_seqs)\n",
    "        # [batch, label_len, dim]\n",
    "        test_embeddings = self.item_embeddings(test)\n",
    "        # gru输出最后的隐层输出当为user embedding\n",
    "        _, user_emb = self.gru(item_seqs_embeddings)\n",
    "        # [batch, dim * gru_num_layers]\n",
    "        user_emb = user_emb.reshape((batch_len, self.gru_num_layers * self.embedding_dim))\n",
    "        # predict\n",
    "        scores = torch.sigmoid(torch.bmm(test_embeddings.repeat([1,1,self.gru_num_layers]), user_emb.unsqueeze(-1)).squeeze())\n",
    "        return scores\n",
    "model = GRU4Rec(num_items = len(itemid2id), embedding_dim = dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0003)\n",
    "criterion = CrossEntropyLoss(reduction='sum').to(device)\n",
    "label = torch.FloatTensor([1 for i in range(pos_num)] + [0 for i in range(neg_sample_num)]).to(device)\n",
    "\n",
    "def DCG(batch_labels):\n",
    "    dcgsum = np.zeros((batch_labels.shape[0]))\n",
    "    for i in range(batch_labels.shape[-1]):\n",
    "        dcg = (2 ** batch_labels[:,i] - 1) / np.math.log(i + 2, 2)\n",
    "        dcgsum += dcg\n",
    "    return dcgsum\n",
    "def NDCG(output, labels):\n",
    "    # ideal_dcg\n",
    "    ideal_dcg = DCG(labels)\n",
    "    # this\n",
    "    dcg = DCG((np.argsort( - output, axis=-1)<pos_num).astype(np.float32))\n",
    "    return np.sum(dcg/ideal_dcg)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        item_seqs = inputs[0].to(device)\n",
    "        test = inputs[1].to(device)\n",
    "        output = model(item_seqs, test)\n",
    "        labels = label.unsqueeze(0).repeat([item_seqs.shape[0],1])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([item_seqs.shape[0], loss.item(), NDCG(output.detach().numpy(), labels.detach().numpy())])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        item_seqs = inputs[0].to(device)\n",
    "        test = inputs[1].to(device)\n",
    "        output = model(item_seqs, test)\n",
    "        labels = label.unsqueeze(0).repeat([item_seqs.shape[0],1])\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_test_losses.append([item_seqs.shape[0], loss.item(), NDCG(output.detach().numpy(), labels.detach().numpy())])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_test_losses])\n",
    "    train_ndcg = sum([x[2] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_ndcg  = sum([x[2] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_ce_loss: {:.4f}, train_ndcg: {:.4f}, validate_ce_loss: {:.4f}, validate_ndcg: {:.4f}'.format(epoch+1, num_epochs,  train_loss, train_ndcg, test_loss, test_ndcg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae55ee82dfec0944",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-02T15:47:43.354772Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-03 00:17:34] epoch=[1/10], train_ce_loss: 0.7470, train_ndcg: 0.6494, validate_ce_loss: 0.7428, validate_ndcg: 0.6644\n",
      "[2023-09-03 00:17:35] epoch=[2/10], train_ce_loss: 0.7412, train_ndcg: 0.6671, validate_ce_loss: 0.7403, validate_ndcg: 0.6676\n",
      "[2023-09-03 00:17:37] epoch=[3/10], train_ce_loss: 0.7392, train_ndcg: 0.6733, validate_ce_loss: 0.7438, validate_ndcg: 0.6666\n",
      "[2023-09-03 00:17:38] epoch=[4/10], train_ce_loss: 0.7381, train_ndcg: 0.6805, validate_ce_loss: 0.7396, validate_ndcg: 0.6584\n",
      "[2023-09-03 00:17:39] epoch=[5/10], train_ce_loss: 0.7376, train_ndcg: 0.6855, validate_ce_loss: 0.7445, validate_ndcg: 0.6578\n",
      "[2023-09-03 00:17:41] epoch=[6/10], train_ce_loss: 0.7383, train_ndcg: 0.6833, validate_ce_loss: 0.7428, validate_ndcg: 0.6577\n",
      "[2023-09-03 00:17:42] epoch=[7/10], train_ce_loss: 0.7385, train_ndcg: 0.6853, validate_ce_loss: 0.7452, validate_ndcg: 0.6579\n",
      "[2023-09-03 00:17:43] epoch=[8/10], train_ce_loss: 0.7397, train_ndcg: 0.6860, validate_ce_loss: 0.7408, validate_ndcg: 0.6587\n",
      "[2023-09-03 00:17:45] epoch=[9/10], train_ce_loss: 0.7365, train_ndcg: 0.6856, validate_ce_loss: 0.7390, validate_ndcg: 0.6605\n",
      "[2023-09-03 00:17:46] epoch=[10/10], train_ce_loss: 0.7364, train_ndcg: 0.6853, validate_ce_loss: 0.7415, validate_ndcg: 0.6626\n"
     ]
    }
   ],
   "source": [
    "# DIN： Deep Interest Network\n",
    "# 阿里妈妈：使用内容特征和基于内容特征之上的行为序列。\n",
    "# y = dnn(user内容特征+行为item序列特征（attention sum）+candidate item内容特征)\n",
    "# 我这里实现没有用物品id的行为嵌入，所以效果不一定好。\n",
    "# 数据集：ml-100k，这里没有考虑连续型特征，故特征总数为2+19\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, CrossEntropyLoss, Sequential, Linear, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=50\n",
    "\n",
    "\n",
    "user_feature_vals = {}\n",
    "for i in range(user_num_features):\n",
    "    user_feature_vals[i] = sorted(list(set([val[i] for val in user_info.values()])))\n",
    "    for user, info in user_info.items():\n",
    "        user_info[user][i] = user_feature_vals[i].index(info[i])\n",
    "item_feature_vals = {}\n",
    "for i in range(item_num_features):\n",
    "    item_feature_vals[i] = sorted(list(set([val[i] for val in item_info.values()])))\n",
    "    for item, info in item_info.items():\n",
    "        item_info[item][i] = item_feature_vals[i].index(info[i])\n",
    "\n",
    "user_profile_data = np.array([user_info[u] for u in data[:,0]]) # [data_len, ufeature]\n",
    "item_seq_profile_data = np.array([[item_info[item] for item in item_seq] for item_seq in data[:,1:]]) # [data_len, seq_len, ufeature]\n",
    "\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len,:]).long(),\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,train_seq_len:(train_seq_len + pos_num + neg_sample_num),:]).long()\n",
    "                                                ), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len + pos_num,:]).long(),\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,-(pos_num + neg_sample_num):,:]).long()\n",
    "                                               ), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "class DIN(nn.Module):\n",
    "    def __init__(self, user_profile_feature: [tuple], item_profile_feature: [tuple], profile_feature_embedding_dim: int, \n",
    "                 dnn_layer_dims: list[int], attention_layer_dims: list[int]):\n",
    "        super(DIN, self).__init__()\n",
    "        # 内容特征\n",
    "        self.user_profile_feature, self.item_profile_feature, self.profile_feature_embedding_dim = user_profile_feature, item_profile_feature, profile_feature_embedding_dim\n",
    "        self.user_profile_embed = nn.ModuleDict({'user_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in user_profile_feature})\n",
    "        self.item_profile_embed = nn.ModuleDict({'item_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in item_profile_feature})\n",
    "        self.user_profile_all_embed_dim = profile_feature_embedding_dim * len(user_profile_feature)\n",
    "        self.item_profile_all_embed_dim = profile_feature_embedding_dim * len(item_profile_feature)\n",
    "        # 注意力 attention net：基于行为特征\n",
    "        self.dnn_layer_dims, self.attention_layer_dims = dnn_layer_dims, attention_layer_dims\n",
    "        self.attention_input_dim = len(user_profile_feature) * profile_feature_embedding_dim + len(item_profile_feature) * profile_feature_embedding_dim + len(user_profile_feature) * len(item_profile_feature)\n",
    "        self.attention_net = nn.Sequential(nn.Linear(self.attention_input_dim, attention_layer_dims[0]))\n",
    "        if len(attention_layer_dims)>1:\n",
    "            for i, layer_dim in enumerate(attention_layer_dims[1:]):\n",
    "                self.attention_net.append(nn.Linear(attention_layer_dims[i], layer_dim))\n",
    "                self.attention_net.append(nn.ReLU())\n",
    "        self.attention_net.append(nn.Linear(attention_layer_dims[-1], 1))\n",
    "        self.attention_net.append(nn.Softmax(dim=-2))\n",
    "        # final dnn\n",
    "        self.all_embedding_dim = len(self.item_profile_feature) * self.profile_feature_embedding_dim * 2\n",
    "        self.final_dnn_network = nn.Sequential(nn.Linear(self.all_embedding_dim, dnn_layer_dims[0]), nn.ReLU())\n",
    "        if len(dnn_layer_dims) > 1:\n",
    "            for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "                self.final_dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "                self.final_dnn_network.append(nn.ReLU())\n",
    "        self.final_dnn_network.append(nn.Linear(dnn_layer_dims[-1], 1))\n",
    "        self.final_dnn_network.append(nn.Sigmoid())\n",
    "    # torch.Tensor([batch, feature]),   torch.Tensor([batch, seq_len, feature]),   torch.Tensor([batch, seq_len, feature])\n",
    "    def forward(self, user_profiles, item_history_list_profile, item_future_list_profile):\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # user profile: [batch, feature * embed_dim]\n",
    "        user_profile_embeddings = torch.cat([self.user_profile_embed['user_embed_' + str(i)](user_profiles[:,i].long()) for i in range(user_profiles.shape[-1])], axis=-1)\n",
    "        user_profile_embeddings = user_profile_embeddings.reshape((batch_len, len(self.user_profile_feature), self.profile_feature_embedding_dim)) # [batch, feature, embed_dim]\n",
    "        # item_history_list_profile: torch.Tensor([batch, seq_len, feature * embed_dim])\n",
    "        seq_len = item_history_list_profile.shape[1]\n",
    "        item_history_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_history_list_profile[:,:,i].long()) for i in range(item_history_list_profile.shape[-1])], axis=-1)\n",
    "        item_history_list_profile_embeddings = item_history_list_profile_embeddings.reshape((batch_len, seq_len, len(self.item_profile_feature), self.profile_feature_embedding_dim)) # [batch, seq_len, feature, embed_dim]\n",
    "        # # attention\n",
    "        a = user_profile_embeddings.unsqueeze(1).repeat((1,seq_len,1,1)).reshape((batch_len * seq_len, len(self.user_profile_feature), self.profile_feature_embedding_dim))\n",
    "        b = item_history_list_profile_embeddings.reshape((batch_len * seq_len, len(self.item_profile_feature), self.profile_feature_embedding_dim))\n",
    "        ab = torch.bmm(a, b.permute(0,2,1)).reshape((batch_len, seq_len, len(self.user_profile_feature) * len(self.item_profile_feature)))\n",
    "        a_ = a.reshape((batch_len, seq_len, len(self.user_profile_feature) * self.profile_feature_embedding_dim))\n",
    "        b_ = b.reshape((batch_len, seq_len, len(self.item_profile_feature) * self.profile_feature_embedding_dim))\n",
    "        # print(a_.shape, b_.shape, ab.shape) # torch.Size([100, 40, 100]) torch.Size([100, 40, 950]) torch.Size([100, 40, 38])\n",
    "        in_attention = torch.cat([a_, b_, ab], dim=-1) # [batch, seq_len, feature, 3 * embed_dim]\n",
    "        # [batch, seq_len, 1] * [batch, seq_len, feature * embed_dim]\n",
    "        out_attention = torch.sum(self.attention_net(in_attention) * item_history_list_profile_embeddings.reshape((batch_len, seq_len, -1)), dim=1) # [batch, feature * embed_dim]\n",
    "\n",
    "        # # 以上处理user profile和行为历史，下面进行与candidate组合预测， item_future_list 和 item_future_list_profile\n",
    "        seq_len_ = item_future_list_profile.shape[1]\n",
    "        item_future_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_future_list_profile[:,:,i].long()) for i in range(item_future_list_profile.shape[-1])], axis=-1)\n",
    "        # print(item_future_list_profile_embeddings.shape, batch_len, seq_len_, len(self.item_profile_feature) * self.profile_feature_embedding_dim) # torch.Size([100, 20, 950]) 100 20 950\n",
    "        item_future_list_profile_embeddings = item_future_list_profile_embeddings.reshape((batch_len, seq_len_, len(self.item_profile_feature) * self.profile_feature_embedding_dim)) # [batch, seq_len, feature * embed_dim]\n",
    "        \n",
    "        x = torch.cat([out_attention.unsqueeze(1).repeat((1,seq_len_,1)), item_future_list_profile_embeddings], dim=-1)\n",
    "        output = self.final_dnn_network(x).squeeze()\n",
    "        return output\n",
    "model = DIN(user_profile_feature = [(i,len(list_)) for i, list_ in user_feature_vals.items()], item_profile_feature= [(i,len(list_)) for i, list_ in item_feature_vals.items()], \n",
    "            profile_feature_embedding_dim = dim, dnn_layer_dims = [16], attention_layer_dims=[16]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0003)\n",
    "criterion = CrossEntropyLoss(reduction='sum').to(device)\n",
    "label = torch.FloatTensor([1 for i in range(pos_num)] + [0 for i in range(neg_sample_num)]).to(device)\n",
    "\n",
    "def DCG(batch_labels):\n",
    "    dcgsum = np.zeros((batch_labels.shape[0]))\n",
    "    for i in range(batch_labels.shape[-1]):\n",
    "        dcg = (2 ** batch_labels[:,i] - 1) / np.math.log(i + 2, 2)\n",
    "        dcgsum += dcg\n",
    "    return dcgsum\n",
    "def NDCG(output, labels):\n",
    "    # ideal_dcg\n",
    "    ideal_dcg = DCG(labels)\n",
    "    # this\n",
    "    dcg = DCG((np.argsort( - output, axis=-1)<pos_num).astype(np.float32))\n",
    "    return np.sum(dcg/ideal_dcg)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # print(item_history_list_profile.shape, item_future_list_profile.shape)\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_test_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_test_losses])\n",
    "    train_ndcg = sum([x[2] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_ndcg  = sum([x[2] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_ce_loss: {:.4f}, train_ndcg: {:.4f}, validate_ce_loss: {:.4f}, validate_ndcg: {:.4f}'.format(epoch+1, num_epochs,  train_loss, train_ndcg, test_loss, test_ndcg))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed56678fc0da646",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T03:27:59.608559Z",
     "start_time": "2023-09-03T03:25:56.842341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-03 11:26:10] epoch=[1/10], train_ce_loss: 0.7461, train_ndcg: 0.6509, validate_ce_loss: 0.7435, validate_ndcg: 0.6702\n",
      "[2023-09-03 11:26:22] epoch=[2/10], train_ce_loss: 0.7398, train_ndcg: 0.6730, validate_ce_loss: 0.7416, validate_ndcg: 0.6630\n",
      "[2023-09-03 11:26:34] epoch=[3/10], train_ce_loss: 0.7386, train_ndcg: 0.6827, validate_ce_loss: 0.7431, validate_ndcg: 0.6596\n",
      "[2023-09-03 11:26:47] epoch=[4/10], train_ce_loss: 0.7376, train_ndcg: 0.6872, validate_ce_loss: 0.7421, validate_ndcg: 0.6648\n",
      "[2023-09-03 11:26:59] epoch=[5/10], train_ce_loss: 0.7366, train_ndcg: 0.6832, validate_ce_loss: 0.7396, validate_ndcg: 0.6614\n",
      "[2023-09-03 11:27:11] epoch=[6/10], train_ce_loss: 0.7362, train_ndcg: 0.6826, validate_ce_loss: 0.7400, validate_ndcg: 0.6607\n",
      "[2023-09-03 11:27:23] epoch=[7/10], train_ce_loss: 0.7358, train_ndcg: 0.6808, validate_ce_loss: 0.7422, validate_ndcg: 0.6628\n",
      "[2023-09-03 11:27:35] epoch=[8/10], train_ce_loss: 0.7353, train_ndcg: 0.6830, validate_ce_loss: 0.7397, validate_ndcg: 0.6604\n",
      "[2023-09-03 11:27:47] epoch=[9/10], train_ce_loss: 0.7361, train_ndcg: 0.6834, validate_ce_loss: 0.7400, validate_ndcg: 0.6614\n",
      "[2023-09-03 11:27:59] epoch=[10/10], train_ce_loss: 0.7358, train_ndcg: 0.6822, validate_ce_loss: 0.7421, validate_ndcg: 0.6634\n"
     ]
    }
   ],
   "source": [
    "# DIEN\n",
    "# Deep Interest Evolution Network for Click-Through Rate Prediction, 2018\n",
    "# 简单实现，适应这个数据集\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, CrossEntropyLoss, Sequential, Linear, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=50\n",
    "\n",
    "user_feature_vals = {}\n",
    "for i in range(user_num_features):\n",
    "    user_feature_vals[i] = sorted(list(set([val[i] for val in user_info.values()])))\n",
    "    for user, info in user_info.items():\n",
    "        user_info[user][i] = user_feature_vals[i].index(info[i])\n",
    "item_feature_vals = {}\n",
    "for i in range(item_num_features):\n",
    "    item_feature_vals[i] = sorted(list(set([val[i] for val in item_info.values()])))\n",
    "    for item, info in item_info.items():\n",
    "        item_info[item][i] = item_feature_vals[i].index(info[i])\n",
    "\n",
    "user_profile_data = np.array([user_info[u] for u in data[:,0]]) # [data_len, ufeature]\n",
    "item_seq_profile_data = np.array([[item_info[item] for item in item_seq] for item_seq in data[:,1:]]) # [data_len, seq_len, ufeature]\n",
    "\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len,:]).long(),\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,train_seq_len:(train_seq_len + pos_num + neg_sample_num),:]).long()\n",
    "                                                ), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len + pos_num,:]).long(),\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,-(pos_num + neg_sample_num):,:]).long()\n",
    "                                               ), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class AUGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AUGRUCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_ih = nn.Linear(input_dim, 3 * hidden_dim).to(device)\n",
    "        self.linear_hh = nn.Linear(hidden_dim, 3 * hidden_dim).to(device)\n",
    "    # [batch, test_len, n_feature * dim], [batch, test_len, dim], [batch, test_len, dim]\n",
    "    def forward(self, inputs, hx, att_score):\n",
    "        gi = self.linear_ih(inputs)\n",
    "        gh = self.linear_hh(hx)\n",
    "        i_r, i_z, i_n = gi[:,:,:self.hidden_dim], gi[:,:,self.hidden_dim:-self.hidden_dim], gi[:,:,-self.hidden_dim:]\n",
    "        h_r, h_z, h_n = gh[:,:,:self.hidden_dim], gi[:,:,self.hidden_dim:-self.hidden_dim], gi[:,:,-self.hidden_dim:]\n",
    "        reset_gate = torch.sigmoid(i_r + h_r)\n",
    "        update_gate = torch.sigmoid(i_z + h_z)\n",
    "        new_state = torch.tanh(i_n + reset_gate * h_n)\n",
    "        update_gate = att_score * update_gate\n",
    "        hy = (1. - update_gate) * hx + update_gate * new_state\n",
    "        return hy\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.linear_hh.parameters()] + [para for para in self.linear_ih.parameters()]\n",
    "class AUGRU(nn.Module):\n",
    "    # n_feature * dim, \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AUGRU, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_net = nn.Sequential(nn.Linear(input_dim*2, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid())\n",
    "        self.agrucell = AUGRUCell(input_dim, hidden_dim).to(device)\n",
    "    # , [batch, seq_len, n_feature * dim], [batch, test_len, n_feature * dim]\n",
    "    # torch.Size([100, 40, 950]) torch.Size([100, 20, 950])\n",
    "    def forward(self, history, target):\n",
    "        batch_len, seq_len = history.shape[0], history.shape[1]\n",
    "        test_len = target.shape[1]\n",
    "        # # [batch, seq_len, test_len, n_feature * dim]  \n",
    "        history_ = history.unsqueeze(-2).repeat((1,1,test_len,1))\n",
    "        target_ = target.unsqueeze(1).repeat((1,seq_len,1,1))\n",
    "        # # [batch, seq_len, test_len, dim]\n",
    "        # torch.Size([100, 40, 20, 950]) torch.Size([100, 40, 20, 950]) 50\n",
    "        attention = self.attention_net(torch.cat([history_, target_], dim=-1))\n",
    "        h = torch.zeros((batch_len, test_len, self.hidden_dim)).to(device) # h0\n",
    "        for i in range(seq_len):\n",
    "            attention_ = attention[:,i,:,:] # [batch, test_len, 1]\n",
    "            # history_[:,i,:,:]: [batch, test_len, n_feature * dim]\n",
    "            # h [batch, test_len, dim]\n",
    "            h = self.agrucell(history_[:,i,:,:], h, attention_)\n",
    "        # [batch_len, test_len, hidden_dim]\n",
    "        return h\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.attention_net.parameters()] + [para for para in self.agrucell.parameters()]\n",
    "class DIEN(nn.Module):\n",
    "    def __init__(self, user_profile_feature: [tuple], item_profile_feature: [tuple], profile_feature_embedding_dim: int, hidden_dim: int,\n",
    "                 dnn_layer_dims: list[int]):\n",
    "        super(DIEN, self).__init__()\n",
    "        # 内容特征\n",
    "        self.user_profile_feature, self.item_profile_feature, self.profile_feature_embedding_dim = user_profile_feature, item_profile_feature, profile_feature_embedding_dim\n",
    "        self.user_profile_embed = nn.ModuleDict({'user_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in user_profile_feature})\n",
    "        self.item_profile_embed = nn.ModuleDict({'item_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in item_profile_feature})\n",
    "        self.user_profile_all_embed_dim = profile_feature_embedding_dim * len(user_profile_feature)\n",
    "        self.item_profile_all_embed_dim = profile_feature_embedding_dim * len(item_profile_feature)\n",
    "        self.dnn_layer_dims, self.hidden_dim = dnn_layer_dims, hidden_dim\n",
    "        # augru\n",
    "        self.augru = AUGRU(profile_feature_embedding_dim * len(item_profile_feature), hidden_dim)\n",
    "        # final dnn\n",
    "        self.all_embedding_dim = hidden_dim + profile_feature_embedding_dim * len(self.item_profile_feature) + profile_feature_embedding_dim * len(self.user_profile_feature)\n",
    "        self.final_dnn_network = nn.Sequential(nn.Linear(self.all_embedding_dim, dnn_layer_dims[0]), nn.ReLU())\n",
    "        if len(dnn_layer_dims) > 1:\n",
    "            for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "                self.final_dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "                self.final_dnn_network.append(nn.ReLU())\n",
    "        self.final_dnn_network.append(nn.Linear(dnn_layer_dims[-1], 1))\n",
    "        self.final_dnn_network.append(nn.Sigmoid())\n",
    "    # torch.Tensor([batch, feature]),   torch.Tensor([batch, seq_len, feature]),   torch.Tensor([batch, seq_len, feature])\n",
    "    def forward(self, user_profiles, item_history_list_profile, item_future_list_profile):\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # user profile: [batch, feature * embed_dim]\n",
    "        user_profile_embeddings = torch.cat([self.user_profile_embed['user_embed_' + str(i)](user_profiles[:,i].long()) for i in range(user_profiles.shape[-1])], axis=-1)\n",
    "        user_profile_embeddings = user_profile_embeddings.reshape((batch_len, len(self.user_profile_feature) * self.profile_feature_embedding_dim)) # [batch, feature, embed_dim]\n",
    "        # item_history_list_profile: torch.Tensor([batch, seq_len, feature * embed_dim])\n",
    "        seq_len = item_history_list_profile.shape[1]\n",
    "        item_history_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_history_list_profile[:,:,i].long()) for i in range(item_history_list_profile.shape[-1])], axis=-1)\n",
    "        item_history_list_profile_embeddings = item_history_list_profile_embeddings.reshape((batch_len, seq_len, len(self.item_profile_feature) * self.profile_feature_embedding_dim)) # [batch, seq_len, feature, embed_dim]\n",
    "        # 以上处理user profile和行为历史，下面进行与candidate组合预测， item_future_list 和 item_future_list_profile\n",
    "        seq_len_ = item_future_list_profile.shape[1]\n",
    "        item_future_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_future_list_profile[:,:,i].long()) for i in range(item_future_list_profile.shape[-1])], axis=-1)\n",
    "        item_future_list_profile_embeddings = item_future_list_profile_embeddings.reshape((batch_len, seq_len_, len(self.item_profile_feature) * self.profile_feature_embedding_dim)) # [batch, seq_len, feature * embed_dim]\n",
    "        # [batch_len, test_len, hidden_dim]\n",
    "        h = self.augru(item_history_list_profile_embeddings, item_future_list_profile_embeddings)\n",
    "        x = torch.cat([h, user_profile_embeddings.unsqueeze(1).repeat((1,seq_len_,1)), item_future_list_profile_embeddings], dim=-1)\n",
    "        output = self.final_dnn_network(x).squeeze() # [batch, seq_len, 1]\n",
    "        return output\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.user_profile_embed.parameters()] + [para for para in self.item_profile_embed.parameters()] + [para for para in self.augru.parameters()] + [para for para in self.final_dnn_network.parameters()]\n",
    "model = DIEN(user_profile_feature = [(i,len(list_)) for i, list_ in user_feature_vals.items()], item_profile_feature= [(i,len(list_)) for i, list_ in item_feature_vals.items()], hidden_dim=dim,\n",
    "            profile_feature_embedding_dim = dim, dnn_layer_dims = [16]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0003)\n",
    "criterion = CrossEntropyLoss(reduction='sum').to(device)\n",
    "label = torch.FloatTensor([1 for i in range(pos_num)] + [0 for i in range(neg_sample_num)]).to(device)\n",
    "\n",
    "def DCG(batch_labels):\n",
    "    dcgsum = np.zeros((batch_labels.shape[0]))\n",
    "    for i in range(batch_labels.shape[-1]):\n",
    "        dcg = (2 ** batch_labels[:,i] - 1) / np.math.log(i + 2, 2)\n",
    "        dcgsum += dcg\n",
    "    return dcgsum\n",
    "def NDCG(output, labels):\n",
    "    # ideal_dcg\n",
    "    ideal_dcg = DCG(labels)\n",
    "    # this\n",
    "    dcg = DCG((np.argsort( - output, axis=-1)<pos_num).astype(np.float32))\n",
    "    return np.sum(dcg/ideal_dcg)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # print(item_history_list_profile.shape, item_future_list_profile.shape)\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_test_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_test_losses])\n",
    "    train_ndcg = sum([x[2] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_ndcg  = sum([x[2] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_ce_loss: {:.4f}, train_ndcg: {:.4f}, validate_ce_loss: {:.4f}, validate_ndcg: {:.4f}'.format(epoch+1, num_epochs,  train_loss, train_ndcg, test_loss, test_ndcg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29c0a47a87673a49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
