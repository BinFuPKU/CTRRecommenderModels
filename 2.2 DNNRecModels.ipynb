{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基于深度学习的推荐模型\n",
    "# DSSM、DeepFM、Deep&Wide、DCN（deepcross）\n",
    "# 、AFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 加载数据集2\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 加载数据\n",
    "ratings = np.array([[int(x) for x in line.strip().split('\\t')[:3]] for line in open('./data/ml-100k/ua.base','r').read().strip().split('\\n')], dtype=np.int32)\n",
    "ratings[:,-1] = (ratings[:,-1] - 0)/(max(ratings[:,-1]) - 0)\n",
    "occupation_dict = {'administrator':0, 'artist':1, 'doctor':2, 'educator':3, 'engineer':4, 'entertainment':5, 'executive':6, 'healthcare':7, 'homemaker':8, 'lawyer':9, 'librarian':10, 'marketing':11, 'none':12, 'other':13, 'programmer':14, 'retired':15, 'salesman':16, 'scientist':17, 'student':18, 'technician':19, 'writer':20}\n",
    "gender_dict={'M':1,'F':0}\n",
    "user_info = {}\n",
    "for line in open('./data/ml-100k/u.user','r', encoding='utf-8').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    user_info[int(phs[0])] = [int(phs[1]), gender_dict[phs[2]], occupation_dict[phs[3]]]\n",
    "item_info = {}\n",
    "for line in open('./data/ml-100k/u.item','r', encoding='ISO-8859-1').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    item_info[int(phs[0])] = phs[5:]\n",
    "# print(len(user_info[list(user_info.keys())[0]]), len(item_info[list(item_info.keys())[0]]))\n",
    "# print(data.shape)\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "num_features = 22"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T02:08:41.419848500Z",
     "start_time": "2023-09-01T02:08:38.315617400Z"
    }
   },
   "id": "232c48c0354b33ac"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 22:06:45] epoch=[1/10], train_mse_loss: 0.3916, validate_mse_loss: 0.2222\n",
      "[2023-08-31 22:06:49] epoch=[2/10], train_mse_loss: 0.1933, validate_mse_loss: 0.1806\n",
      "[2023-08-31 22:06:54] epoch=[3/10], train_mse_loss: 0.1747, validate_mse_loss: 0.1736\n",
      "[2023-08-31 22:06:59] epoch=[4/10], train_mse_loss: 0.1707, validate_mse_loss: 0.1713\n",
      "[2023-08-31 22:07:03] epoch=[5/10], train_mse_loss: 0.1690, validate_mse_loss: 0.1700\n",
      "[2023-08-31 22:07:08] epoch=[6/10], train_mse_loss: 0.1678, validate_mse_loss: 0.1688\n",
      "[2023-08-31 22:07:13] epoch=[7/10], train_mse_loss: 0.1667, validate_mse_loss: 0.1678\n",
      "[2023-08-31 22:07:18] epoch=[8/10], train_mse_loss: 0.1659, validate_mse_loss: 0.1670\n",
      "[2023-08-31 22:07:23] epoch=[9/10], train_mse_loss: 0.1652, validate_mse_loss: 0.1664\n",
      "[2023-08-31 22:07:27] epoch=[10/10], train_mse_loss: 0.1646, validate_mse_loss: 0.1659\n"
     ]
    }
   ],
   "source": [
    "# DSSM: Deep Structured Semantic Model  \n",
    "# y = cosine(mlp_u(user_feature), mlp_i(item_feature))\n",
    "# Learning deep structured semantic models for web search using clickthrough data\n",
    "# 2013年微软\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, MSELoss, Sequential, Linear, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=100\n",
    "\n",
    "data = np.array([user_info[u] + item_info[i] + [r] for u, i, r in ratings], dtype=np.float32)\n",
    "data[:,:-1] = MinMaxScaler().fit_transform(data[:,:-1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.4, random_state=0)\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class DSSM(Module):\n",
    "    def __init__(self, user_layer_dims, item_layer_dims):\n",
    "        super(DSSM, self).__init__()\n",
    "        self.user_layer_dims = user_layer_dims\n",
    "        self.item_layer_dims = item_layer_dims\n",
    "        # user dnn\n",
    "        self.user_dnn = Sequential()\n",
    "        for i, layer_dim in enumerate(user_layer_dims[1:]):\n",
    "            self.user_dnn.append(Linear(user_layer_dims[i], layer_dim))\n",
    "            self.user_dnn.append(Sigmoid())\n",
    "        # item dnn\n",
    "        self.item_dnn = Sequential()\n",
    "        for i, layer_dim in enumerate(item_layer_dims[1:]):\n",
    "            self.item_dnn.append(Linear(item_layer_dims[i], layer_dim))\n",
    "            self.item_dnn.append(Sigmoid())\n",
    "    def forward(self, user_features, item_features):\n",
    "        return torch.cosine_similarity(self.user_dnn(user_features), self.item_dnn(item_features), dim=-1)\n",
    "model = DSSM(user_layer_dims=[3, 8], item_layer_dims=[19, 8]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input[:,:3], input[:,3:])\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input[:,:3], input[:,3:])\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T14:07:27.816404Z",
     "start_time": "2023-08-31T14:06:39.565561Z"
    }
   },
   "id": "edad18db26a18c6a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 10:09:08] epoch=[1/10], train_mse_loss: 0.1634, validate_mse_loss: 0.1635\n",
      "[2023-09-01 10:09:15] epoch=[2/10], train_mse_loss: 0.1621, validate_mse_loss: 0.1628\n",
      "[2023-09-01 10:09:23] epoch=[3/10], train_mse_loss: 0.1614, validate_mse_loss: 0.1620\n",
      "[2023-09-01 10:09:31] epoch=[4/10], train_mse_loss: 0.1607, validate_mse_loss: 0.1618\n",
      "[2023-09-01 10:09:39] epoch=[5/10], train_mse_loss: 0.1602, validate_mse_loss: 0.1620\n",
      "[2023-09-01 10:09:46] epoch=[6/10], train_mse_loss: 0.1596, validate_mse_loss: 0.1602\n",
      "[2023-09-01 10:09:54] epoch=[7/10], train_mse_loss: 0.1592, validate_mse_loss: 0.1606\n",
      "[2023-09-01 10:10:01] epoch=[8/10], train_mse_loss: 0.1590, validate_mse_loss: 0.1597\n",
      "[2023-09-01 10:10:10] epoch=[9/10], train_mse_loss: 0.1584, validate_mse_loss: 0.1595\n",
      "[2023-09-01 10:10:18] epoch=[10/10], train_mse_loss: 0.1582, validate_mse_loss: 0.1617\n"
     ]
    }
   ],
   "source": [
    "# Wide & Deep: y = simoid(dnn(x), x)\n",
    "# 2016年youtube Wide & deep learning for recommender systems[C]//Proceedings of the 1st Workshop on Deep Learning for Recommender Systems\n",
    "# 数据集：ml-100k\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Parameter, MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.nn import Module, Sequential, ReLU, Dropout, Sigmoid\n",
    "\n",
    "# category: [1,2] + [all]\n",
    "# number: [0] + []\n",
    "number_feature_data = MinMaxScaler().fit_transform(np.array([[user_info[u][0]]  for u, i, r in ratings], dtype=np.float32))\n",
    "category_feature_data = np.array([user_info[u][1:] + item_info[i] for u, i, r in ratings], dtype=np.int32)\n",
    "data = np.concatenate([number_feature_data, category_feature_data, ratings[:,-1:]], axis=-1)\n",
    "num_number_features = number_feature_data.shape[-1]\n",
    "num_category_features = category_feature_data.shape[-1]\n",
    "num_features = data.shape[-1] - 1\n",
    "category_feature_vals = {}\n",
    "for i in range(num_number_features, num_features):\n",
    "    category_feature_vals[i] = sorted(list(set(list(data[:, i]))))\n",
    "    for rid in range(data.shape[0]):\n",
    "        data[rid, i] = category_feature_vals[i].index(data[rid, i])\n",
    "# print(len(user_info[list(user_info.keys())[0]]), len(item_info[list(item_info.keys())[0]]))\n",
    "# print(data.shape)\n",
    "\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "embedding_dim = 8\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.4, random_state=0)\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class WideDeep(nn.Module):\n",
    "    def __init__(self, dense_feature_cols, sparse_feature_col_dims, dnn_layer_dims, dnn_dropout=0.):\n",
    "        super(WideDeep, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_col_dims = dense_feature_cols, sparse_feature_col_dims\n",
    "        # sparse feature embedding dict\n",
    "        self.embed_layers = nn.ModuleDict({'embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=dim) for i, valcount, dim in sparse_feature_col_dims})\n",
    "        # dnn part\n",
    "        dnn_layer_dims.insert(0, len(dense_feature_cols) + sum([x[-1] for x in sparse_feature_col_dims]))\n",
    "        self.dnn_network = Sequential()\n",
    "        for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "            self.dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "            self.dnn_network.append(ReLU())\n",
    "        self.dnn_network.append(Dropout(dnn_dropout))\n",
    "        # linear + final layer\n",
    "        self.final_nn = Sequential(nn.Linear(dnn_layer_dims[-1] + dnn_layer_dims[0], 1), Sigmoid())\n",
    "    def forward(self, x):\n",
    "        dense_input = x[:, :len(self.dense_feature_cols)]\n",
    "        sparse_embeds = torch.cat([self.embed_layers['embed_' + str(i)](x[:, i].long()) for i in range(len(self.dense_feature_cols), x.shape[1])], axis=-1)\n",
    "        x = torch.cat([sparse_embeds, dense_input], axis=-1)\n",
    "        # dnn part\n",
    "        deep_out = self.dnn_network(x)\n",
    "        #  Concatenate and out\n",
    "        outputs = self.final_nn(torch.cat([x, deep_out], axis=-1))\n",
    "        return outputs.squeeze()\n",
    "model = WideDeep(dense_feature_cols=[i for i in range(num_number_features)], sparse_feature_col_dims=[(i,len(category_feature_vals[i]), embedding_dim) for i in range(num_number_features, num_features)], dnn_layer_dims=[128, 32], dnn_dropout=0.).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T02:10:18.102528200Z",
     "start_time": "2023-09-01T02:08:55.507579700Z"
    }
   },
   "id": "fd69c1e07310ea2a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 11:05:57] epoch=[1/10], train_mse_loss: 0.1643, validate_mse_loss: 0.1634\n",
      "[2023-09-01 11:07:04] epoch=[2/10], train_mse_loss: 0.1623, validate_mse_loss: 0.1628\n",
      "[2023-09-01 11:08:10] epoch=[3/10], train_mse_loss: 0.1620, validate_mse_loss: 0.1627\n",
      "[2023-09-01 11:09:14] epoch=[4/10], train_mse_loss: 0.1614, validate_mse_loss: 0.1618\n",
      "[2023-09-01 11:10:19] epoch=[5/10], train_mse_loss: 0.1607, validate_mse_loss: 0.1614\n",
      "[2023-09-01 11:11:24] epoch=[6/10], train_mse_loss: 0.1604, validate_mse_loss: 0.1624\n",
      "[2023-09-01 11:12:29] epoch=[7/10], train_mse_loss: 0.1598, validate_mse_loss: 0.1605\n",
      "[2023-09-01 11:13:35] epoch=[8/10], train_mse_loss: 0.1593, validate_mse_loss: 0.1602\n",
      "[2023-09-01 11:14:40] epoch=[9/10], train_mse_loss: 0.1592, validate_mse_loss: 0.1602\n",
      "[2023-09-01 11:15:46] epoch=[10/10], train_mse_loss: 0.1590, validate_mse_loss: 0.1603\n"
     ]
    }
   ],
   "source": [
    "# DeepCross： DCN\n",
    "# 和Wide&Deep相比，去掉Wide部分，DNN部分换成残差网络，模型结构简单\n",
    "# x = f_i f_j * w1 + w0 + x 残差网络迭代多次\n",
    "# y = sigmoid(linear(x))\n",
    "# [ACM kdd 2016] Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Parameter, MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.nn import Module, Sequential, ReLU, Dropout, Sigmoid\n",
    "\n",
    "# category: [1,2] + [all]\n",
    "# number: [0] + []\n",
    "number_feature_data = MinMaxScaler().fit_transform(np.array([[user_info[u][0]]  for u, i, r in ratings], dtype=np.float32))\n",
    "category_feature_data = np.array([user_info[u][1:] + item_info[i] for u, i, r in ratings], dtype=np.int32)\n",
    "data = np.concatenate([number_feature_data, category_feature_data, ratings[:,-1:]], axis=-1)\n",
    "num_number_features = number_feature_data.shape[-1]\n",
    "num_category_features = category_feature_data.shape[-1]\n",
    "num_features = data.shape[-1] - 1\n",
    "category_feature_vals = {}\n",
    "for i in range(num_number_features, num_features):\n",
    "    category_feature_vals[i] = sorted(list(set(list(data[:, i]))))\n",
    "    for rid in range(data.shape[0]):\n",
    "        data[rid, i] = category_feature_vals[i].index(data[rid, i])\n",
    "# print(len(user_info[list(user_info.keys())[0]]), len(item_info[list(item_info.keys())[0]]))\n",
    "# print(data.shape)\n",
    "\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "embedding_dim = 8\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.4, random_state=0)\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class CrossNetwork(nn.Module):\n",
    "    def __init__(self, layer_num, input_dim):\n",
    "        super(CrossNetwork, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.cross_weights = nn.ParameterList([nn.Parameter(torch.rand(input_dim, 1)) for i in range(self.layer_num)])\n",
    "        self.cross_bias = nn.ParameterList([nn.Parameter(torch.rand(input_dim, 1)) for i in range(self.layer_num)])\n",
    "    def forward(self, x):\n",
    "        x_ = torch.unsqueeze(x.clone(), dim=2)\n",
    "        for i in range(self.layer_num):\n",
    "            x_ = torch.matmul(torch.bmm(x_, x_.permute((0, 2, 1))), self.cross_weights[i]) + self.cross_bias[i] + x_\n",
    "        return torch.squeeze(torch.sigmoid(x_))\n",
    "\n",
    "class DeepCross(nn.Module):\n",
    "    def __init__(self, dense_feature_cols, sparse_feature_col_dims, dnn_layer_dims, cross_layer_num, dnn_dropout=0.):\n",
    "        super(DeepCross, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_col_dims = dense_feature_cols, sparse_feature_col_dims\n",
    "        # sparse feature embedding dict\n",
    "        self.embed_layers = nn.ModuleDict({'embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=dim) for i, valcount, dim in sparse_feature_col_dims})\n",
    "        # Dnn part: sparse features\n",
    "        dnn_layer_dims.insert(0, len(dense_feature_cols) + sum([x[-1] for x in sparse_feature_col_dims]))\n",
    "        self.dnn_network = Sequential()\n",
    "        for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "            self.dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "            self.dnn_network.append(ReLU())\n",
    "        self.dnn_network.append(Dropout(dnn_dropout))\n",
    "        # cross part\n",
    "        self.cross_network = CrossNetwork(cross_layer_num, dnn_layer_dims[0])\n",
    "        # final layer\n",
    "        self.final_nn = Sequential(nn.Linear(dnn_layer_dims[-1] + dnn_layer_dims[0], 1), Sigmoid())\n",
    "    # [batch, [dense, sparse]]\n",
    "    def forward(self, x):\n",
    "        dense_input = x[:, :len(self.dense_feature_cols)]\n",
    "        sparse_embeds = torch.cat([self.embed_layers['embed_' + str(i)](x[:, i].long()) for i in range(len(self.dense_feature_cols), x.shape[1])], axis=-1)\n",
    "        x = torch.cat([sparse_embeds, dense_input], axis=-1)\n",
    "        # cross Network\n",
    "        cross_out = self.cross_network(x)\n",
    "        # Deep Network\n",
    "        deep_out = self.dnn_network(x)\n",
    "        #  Concatenate and out\n",
    "        outputs = self.final_nn(torch.cat([cross_out, deep_out], axis=-1))\n",
    "        return outputs.squeeze()\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.embed_layers.parameters()] + [para for para in self.dnn_network.parameters()] \\\n",
    "             + [para for para in self.cross_network.parameters()] + [para for para in self.final_nn.parameters()]\n",
    "model = DeepCross(dense_feature_cols=[i for i in range(num_number_features)], sparse_feature_col_dims=[(i,len(category_feature_vals[i]), embedding_dim) for i in range(num_number_features, num_features)], dnn_layer_dims=[128, 32], cross_layer_num=2, dnn_dropout=0.).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:15:46.084561200Z",
     "start_time": "2023-09-01T03:04:50.768705500Z"
    }
   },
   "id": "54c8eab05744fd44"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 11:16:10] epoch=[1/10], train_mse_loss: 0.2199, validate_mse_loss: 0.2115\n",
      "[2023-09-01 11:16:20] epoch=[2/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:16:30] epoch=[3/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:16:42] epoch=[4/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:16:54] epoch=[5/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:17:07] epoch=[6/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:17:19] epoch=[7/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:17:29] epoch=[8/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:17:39] epoch=[9/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n",
      "[2023-09-01 11:17:49] epoch=[10/10], train_mse_loss: 0.2096, validate_mse_loss: 0.2113\n"
     ]
    }
   ],
   "source": [
    "# DeepFM: y = sigmoid(dnn(x) + fm(x_discrete))，使用FM替换了Wide & Deep模型中wide部分的LR\n",
    "# Deepfm: a factorization-machine based neural network for ctr prediction\n",
    "# FM + DNN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Parameter, MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.nn import Module, Sequential, ReLU, Dropout, Sigmoid\n",
    "\n",
    "# category: [1,2] + [all]\n",
    "# number: [0] + []\n",
    "number_feature_data = MinMaxScaler().fit_transform(np.array([[user_info[u][0]]  for u, i, r in ratings], dtype=np.float32))\n",
    "category_feature_data = np.array([user_info[u][1:] + item_info[i] for u, i, r in ratings], dtype=np.int32)\n",
    "data = np.concatenate([number_feature_data, category_feature_data, ratings[:,-1:]], axis=-1)\n",
    "num_number_features = number_feature_data.shape[-1]\n",
    "num_category_features = category_feature_data.shape[-1]\n",
    "num_features = data.shape[-1] - 1\n",
    "category_feature_vals = {}\n",
    "for i in range(num_number_features, num_features):\n",
    "    category_feature_vals[i] = sorted(list(set(list(data[:, i]))))\n",
    "    for rid in range(data.shape[0]):\n",
    "        data[rid, i] = category_feature_vals[i].index(data[rid, i])\n",
    "# print(len(user_info[list(user_info.keys())[0]]), len(item_info[list(item_info.keys())[0]]))\n",
    "# print(data.shape)\n",
    "\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "embedding_dim = 8 # sparse feature embedding dim\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-1], data[:,-1], test_size=0.4, random_state=0)\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# vector-wise cross FM仅对离散特征进行交叉, 输出没有sigmoid，留到最后做\n",
    "class FactorizationMachine(Module):\n",
    "    def __init__(self, num_features, dim):\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.dim = dim\n",
    "        self.w0 = Parameter(torch.randn((1,1)), requires_grad=True)\n",
    "        self.w = Parameter(torch.randn((num_features, 1)), requires_grad=True)\n",
    "        self.bw = Parameter(torch.randn((num_features, dim)), requires_grad=True)\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        X_ = X.reshape([-1, self.num_features, self.dim])\n",
    "        # 二阶\n",
    "        X_ = torch.bmm(X_, X_.permute([0,2,1]))\n",
    "        tmp = torch.matmul(self.bw, self.bw.T)\n",
    "        tmp[np.tril_indices(self.num_features)] = 0\n",
    "        X_ = X_ * tmp.unsqueeze(0)\n",
    "        two = torch.sum(torch.sum(X_, dim=-1, keepdim=False), dim=-1, keepdim=True)\n",
    "        # 一阶\n",
    "        one = torch.matmul(X, self.w.repeat([self.dim,1]))\n",
    "        return (self.w0 + one + two).squeeze()\n",
    "\n",
    "class DeepFM(Module):\n",
    "    def __init__(self, dense_feature_cols, sparse_feature_cols, sparse_feature_embedding_dim, dnn_layer_dims, dnn_dropout=0.):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols, self.sparse_feature_embedding_dim = dense_feature_cols, sparse_feature_cols, sparse_feature_embedding_dim\n",
    "        # fm\n",
    "        self.discrete_feature_FM = FactorizationMachine(len(sparse_feature_cols), sparse_feature_embedding_dim)\n",
    "        # sparse feature embedding dict\n",
    "        self.embed_layers = nn.ModuleDict({'embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=sparse_feature_embedding_dim) for i, valcount in sparse_feature_cols})\n",
    "        # dnn part\n",
    "        dnn_layer_dims.insert(0, len(dense_feature_cols) + sparse_feature_embedding_dim * len(sparse_feature_cols))\n",
    "        self.dnn_network = Sequential()\n",
    "        for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "            self.dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "            self.dnn_network.append(ReLU())\n",
    "        self.dnn_network.append(Dropout(dnn_dropout))\n",
    "        self.dnn_network.append(nn.Linear(dnn_layer_dims[-1], 1))\n",
    "    def forward(self, x):\n",
    "        dense_input = x[:, :len(self.dense_feature_cols)]\n",
    "        sparse_embeds = torch.cat([self.embed_layers['embed_' + str(i)](x[:, i].long()) for i in range(len(self.dense_feature_cols), x.shape[1])], axis=1)\n",
    "        x = torch.cat([sparse_embeds, dense_input], axis=-1)\n",
    "        # FM仅对离散特征进行交叉\n",
    "        fm_out = self.discrete_feature_FM(sparse_embeds)\n",
    "        # dnn part\n",
    "        deep_out = self.dnn_network(x).squeeze()\n",
    "        y = torch.sigmoid(fm_out + deep_out).squeeze() # 最后sigmoid\n",
    "        return y\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.discrete_feature_FM.parameters()] + [para for para in self.embed_layers.parameters()] \\\n",
    "             + [para for para in self.dnn_network.parameters()]\n",
    "    \n",
    "model = DeepFM(dense_feature_cols=[i for i in range(num_number_features)], sparse_feature_cols=[(i,len(category_feature_vals[i])) for i in range(num_number_features, num_features)], sparse_feature_embedding_dim =embedding_dim, dnn_layer_dims=[128, 32], dnn_dropout=0.).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = MSELoss(reduction='sum').to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([input.shape[0], loss.item()])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        input = inputs[0].to(device)\n",
    "        label = inputs[1].to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        epoch_test_losses.append([input.shape[0], loss.item()])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_mse_loss: {:.4f}, validate_mse_loss: {:.4f}'.format(epoch+1, num_epochs,  train_loss, test_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:17:49.559529200Z",
     "start_time": "2023-09-01T03:15:56.187996600Z"
    }
   },
   "id": "bfc69ea7d477957"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AFM : Attentive Factorization Machine\n",
    "# y = w0 + w1 * f + a * (w2, w2) * (f, f)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AFM(nn.Module):\n",
    "    def __init__(self, dense_feature_cols, sparse_feature_cols, sparse_feature_embedding_dim, attention_layer_dim):\n",
    "        super(AFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols, self.sparse_feature_embedding_dim = dense_feature_cols, sparse_feature_cols, sparse_feature_embedding_dim\n",
    "        self.attention_layer_dim = attention_layer_dim\n",
    "        self.attention_layer = nn.Sequential(nn.Linear(sparse_feature_embedding_dim, attention_layer_dim), nn.ReLU(), nn.Linear(attention_layer_dim, 1), nn.Softmax(dim=-1))\n",
    "        # sparse feature embedding dict\n",
    "        self.embed_layers = nn.ModuleDict({'embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=sparse_feature_embedding_dim) for i, valcount in sparse_feature_cols})\n",
    "        # final layer\n",
    "        self.final_layer = nn.Sequential(Linear(len(sparse_feature_cols) * sparse_feature_embedding_dim + len(dense_feature_cols), 1), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        dense_input = x[:, :len(self.dense_feature_cols)]\n",
    "        sparse_embeds = torch.cat([self.embed_layers['embed_' + str(i)](x[:, i].long()) for i in range(len(self.dense_feature_cols), x.shape[1])], axis=1)\n",
    "        x = torch.cat([sparse_embeds, dense_input], axis=-1)\n",
    "        # FM仅对离散特征进行交叉\n",
    "        fm_out = self.discrete_feature_FM(sparse_embeds)\n",
    "        # dnn part\n",
    "        deep_out = self.dnn_network(x).squeeze()\n",
    "        y = torch.sigmoid(fm_out + deep_out).squeeze() # 最后sigmoid\n",
    "        return y\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.discrete_feature_FM.parameters()] + [para for para in self.embed_layers.parameters()] \\\n",
    "             + [para for para in self.dnn_network.parameters()]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5408044f05d560fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
