{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 多兴趣建模：大部分模型将⽤户的兴趣表达为⼀个 user embedding，然而单个embedding来表达⽤户的多种兴趣是很困难的。尤其是⻓期⾏为序列推荐的场景。\n",
    "# 可用于多兴趣召回。\n",
    "# MIND、ComirecSA、SINE、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 1548\n",
      "(446, 81)\n",
      "[[ 397  303  260  306  312  744  257  285  338  270  682  862  328 1543\n",
      "   344  881  326  298  867  265  673 1491  301  337  353  261  300 1260\n",
      "  1238  302  325  334  331  351  347 1090  683  901  897  272  345   49\n",
      "   585  309  521  126  386 1282 1038  539  288  417  418  931 1444  804\n",
      "   164  933  941 1326  686 1001  316  324 1128  900 1091 1349  710  716\n",
      "   470 1499  225   98 1508  357  365  757  887  248  633]\n",
      " [ 344  773  365  397  445   48  374   62  431  981  231  780  384  109\n",
      "    39  775 1021  929 1022  393   93  399   89  386  569  715   66 1059\n",
      "   748  414  459  418  139  832  495  831 1413  413  398  396   77  784\n",
      "   717  786   50  142  768  259 1159  394   11  917  793 1306 1307  810\n",
      "   302  944 1328  564   53  293  141  212  778  645  825  839  328  332\n",
      "  1234 1363  342 1247  485  107 1134 1137  369 1277  254]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# 加载数据集2成序列数据集，评分[0,1,2]为负反馈，评分[3,4,5]为正反馈，只保留正样本，构造简单序列推荐数据集\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "random.seed(100)\n",
    "\n",
    "# 加载数据: >=3分为正，用户评分次数不低于50，只保留最后50个，拆分为40: 5 + 15负例 (随机采样): 5 + 15负例 (随机采样)\n",
    "ratings = np.array([[int(x) for x in line.strip().split('\\t')[:4]] for line in open('./data/ml-100k/ua.base','r').read().strip().split('\\n')], dtype=np.int32)\n",
    "ratings_pd = pd.DataFrame({feature_name: list(feature_data) for feature_name, feature_data in zip(['user_id','item_id','rating','timestamp'], ratings.T)})\n",
    "pos_ratings_pd = ratings_pd[ratings_pd['rating']>2.9][['user_id','item_id','timestamp']].dropna().sort_values('timestamp') # 已经排序了\n",
    "pos_ratings_pd = pos_ratings_pd.groupby('user_id').filter(lambda x: x['user_id'].count()>=50)\n",
    "userid2id = {user_id: i for i, user_id in enumerate(sorted(list(set(pos_ratings_pd['user_id'].tolist()))))}\n",
    "itemid2id = {item_id: i for i, item_id in enumerate(sorted(list(set(pos_ratings_pd['item_id'].tolist()))))}\n",
    "print(len(userid2id), len(itemid2id))\n",
    "del ratings, ratings_pd\n",
    "\n",
    "# new id\n",
    "user_train_validate_test = {}\n",
    "for user,item,t in pos_ratings_pd.values:\n",
    "    u, i = userid2id[user], itemid2id[item]\n",
    "    if u not in user_train_validate_test:\n",
    "        user_train_validate_test[u] = [i]\n",
    "    else:\n",
    "        user_train_validate_test[u].append(i)\n",
    "    user_train_validate_test[u] = user_train_validate_test[u][-50:]\n",
    "train_seq_len = 40\n",
    "pos_num = 5\n",
    "neg_sample_num = 15\n",
    "def sample(low, high, notinset, num):\n",
    "    nums = set([])\n",
    "    n = num\n",
    "    while n>0:\n",
    "        id = random.randint(low, high)\n",
    "        if id not in notinset and id not in nums:\n",
    "            nums.add(id)\n",
    "            n -= 1\n",
    "    return list(nums)\n",
    "data = np.zeros((len(user_train_validate_test), 81), dtype=np.int32)\n",
    "i = 0\n",
    "for user, train_validate_test in user_train_validate_test.items():\n",
    "    train, validate, test = train_validate_test[:train_seq_len], train_validate_test[-pos_num*2:-pos_num], train_validate_test[-pos_num:]\n",
    "    data[i, 0] = user\n",
    "    data[i,1:train_seq_len+1] = np.array(train)\n",
    "    samples = sample(0, len(itemid2id)-1, set(train_validate_test), neg_sample_num * 2)\n",
    "    data[i,1+train_seq_len : 1+train_seq_len+pos_num+neg_sample_num] = np.array(validate + samples[:neg_sample_num])\n",
    "    data[i,1+train_seq_len+pos_num+neg_sample_num : ] = np.array(test + samples[neg_sample_num:])\n",
    "    i += 1\n",
    "del user_train_validate_test\n",
    "print(data.shape)\n",
    "print(data[:2,:])\n",
    "\n",
    "# 继续加载info特征信息，内容特征\n",
    "occupation_dict = {'administrator':0, 'artist':1, 'doctor':2, 'educator':3, 'engineer':4, 'entertainment':5, 'executive':6, 'healthcare':7, 'homemaker':8, 'lawyer':9, 'librarian':10, 'marketing':11, 'none':12, 'other':13, 'programmer':14, 'retired':15, 'salesman':16, 'scientist':17, 'student':18, 'technician':19, 'writer':20}\n",
    "gender_dict={'M':1,'F':0}\n",
    "user_info = {}\n",
    "for line in open('./data/ml-100k/u.user','r', encoding='utf-8').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    if int(phs[0]) not in userid2id:\n",
    "        continue\n",
    "    uid = userid2id[int(phs[0])]\n",
    "    user_info[uid] = [gender_dict[phs[2]], occupation_dict[phs[3]]] # int(phs[1]) 为了方便，不要连续型特征\n",
    "user_num_features = 2\n",
    "item_info = {}\n",
    "for line in open('./data/ml-100k/u.item','r', encoding='ISO-8859-1').read().strip().split('\\n'):\n",
    "    phs = line.strip().split('|')\n",
    "    if int(phs[0]) not in itemid2id:\n",
    "        continue\n",
    "    iid = itemid2id[int(phs[0])]\n",
    "    item_info[iid] = phs[5:]\n",
    "item_num_features = 19\n",
    "num_users = len(user_info)\n",
    "num_items = len(item_info)\n",
    "num_features = 21\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T02:49:19.833813700Z",
     "start_time": "2023-09-05T02:49:17.373532700Z"
    }
   },
   "id": "c5a7915fd42957cc"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-05 11:39:33] epoch=[1/10], train_ce_loss: 0.7489, train_ndcg: 0.6662, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:39:38] epoch=[2/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:39:43] epoch=[3/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:39:48] epoch=[4/10], train_ce_loss: 0.7489, train_ndcg: 0.6640, validate_ce_loss: 0.7489, validate_ndcg: 0.6677\n",
      "[2023-09-05 11:39:53] epoch=[5/10], train_ce_loss: 0.7489, train_ndcg: 0.6688, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:39:58] epoch=[6/10], train_ce_loss: 0.7489, train_ndcg: 0.6682, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:40:04] epoch=[7/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:40:09] epoch=[8/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:40:15] epoch=[9/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 11:40:20] epoch=[10/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n"
     ]
    }
   ],
   "source": [
    "# MIND： Multi-interest network with dynamic routing for recommendation at Tmall\n",
    "# 采⽤了hinton提出的胶囊⽹络(动态路由)作为多兴趣提取层（指定胶囊个数，可以类似于K-Means聚类来理解）\n",
    "# 我这里用了user_profile和item_profile，基于item_profile的行为序列，采用胶囊网络来建模行为序列的多兴趣偏好。\n",
    "# 胶囊网络迭代过程中增大top-1兴趣节点的激活，降低其他兴趣节点的激活，本质类似聚类switch expert。\n",
    "# 这种是hard选择路由。\n",
    "# 多头注意力则是soft路由。\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, CrossEntropyLoss, Sequential, Linear, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=50\n",
    "\n",
    "user_feature_vals = {}\n",
    "for i in range(user_num_features):\n",
    "    user_feature_vals[i] = sorted(list(set([val[i] for val in user_info.values()])))\n",
    "    for user, info in user_info.items():\n",
    "        user_info[user][i] = user_feature_vals[i].index(info[i])\n",
    "item_feature_vals = {}\n",
    "for i in range(item_num_features):\n",
    "    item_feature_vals[i] = sorted(list(set([val[i] for val in item_info.values()])))\n",
    "    for item, info in item_info.items():\n",
    "        item_info[item][i] = item_feature_vals[i].index(info[i])\n",
    "\n",
    "user_profile_data = np.array([user_info[u] for u in data[:,0]]) # [data_len, ufeature]\n",
    "item_seq_profile_data = np.array([[item_info[item] for item in item_seq] for item_seq in data[:,1:]]) # [data_len, seq_len, ufeature]\n",
    "\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len,:]).long(),\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,train_seq_len:(train_seq_len + pos_num + neg_sample_num),:]).long()\n",
    "                                                ), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len,:]).long(), # 这里图简便，懒得改seq_len了\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,-(pos_num + neg_sample_num):,:]).long()\n",
    "                                               ), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 胶囊网络结构\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, seq_len: int, num_interests: int = 4, routing_times: int = 3):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.hidden_dim, self.seq_len = hidden_dim, seq_len\n",
    "        self.num_interests, self.routing_times = num_interests, routing_times\n",
    "        self.to_interest_linear = nn.Linear(hidden_dim, hidden_dim * num_interests, bias=False)\n",
    "        self.relu_linear = nn.Sequential(nn.Linear(hidden_dim, hidden_dim, bias=False), nn.ReLU())\n",
    "    # [batch_len, seq_len, profile_embedding], [batch_len, seq_len]\n",
    "    def forward(self, history_item_embeddings, mask):\n",
    "        batch_len = history_item_embeddings.shape[0]\n",
    "        # 计算u, [batch_len, interest_num, seq_len, hidden_dim]\n",
    "        interest_item_embeddings = self.to_interest_linear(history_item_embeddings)\n",
    "        interest_item_embeddings = interest_item_embeddings.reshape((batch_len, self.seq_len, self.num_interests, self.hidden_dim))\n",
    "        interest_item_embeddings = interest_item_embeddings.permute((0,2,1,3))\n",
    "        # 随机初始化胶囊权重b\n",
    "        capsule_weight = torch.randn((batch_len, self.num_interests, self.seq_len), device=device, requires_grad=False)\n",
    "        # 动态路由传播3次\n",
    "        for i in range(self.routing_times):\n",
    "            # mask，最后shape=[b, in, 1, s]\n",
    "            atten_mask = torch.unsqueeze(mask, 1).repeat(1, self.num_interests, 1)\n",
    "            paddings = torch.zeros_like(atten_mask, dtype=torch.float)\n",
    "            # 计算c\n",
    "            capsule_softmax_weight = torch.softmax(capsule_weight, dim=-1)\n",
    "            capsule_softmax_weight = torch.where(torch.eq(atten_mask, 0), paddings, capsule_softmax_weight)  # mask位置填充0\n",
    "            capsule_softmax_weight = torch.unsqueeze(capsule_softmax_weight, 2)\n",
    "            if i <= 1: # 前两次\n",
    "                # 计算s\n",
    "                # [batch_len, num_interests, 1, seq_len]  [batch_len, num_interests, seq_len, hidden_dim] -> [batch_len, num_interests, 1, hidden_dim]\n",
    "                interest_capsule = torch.matmul(capsule_softmax_weight, interest_item_embeddings)\n",
    "                # 计算v = squash(s)\n",
    "                cap_norm = torch.sum(interest_capsule.square(), -1, keepdim=True)  # [batch_len, num_interests, 1, 1]\n",
    "                scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)\n",
    "                interest_capsule = scalar_factor * interest_capsule  # [batch_len, num_interests, 1, hidden_dim]\n",
    "                # 计算b = b + u * v\n",
    "                # [batch_len, interest_num, seq_len, hidden_dim], [batch_len, num_interests, hidden_dim, 1] -> [batch_len, interest_num, seq_len, 1]\n",
    "                delta_weight = torch.matmul(interest_item_embeddings, interest_capsule.transpose(2, 3).contiguous())\n",
    "                delta_weight = delta_weight.squeeze()\n",
    "                # 更新时候，正益正，负益负（向量点积作用），迭代多次。\n",
    "                capsule_weight = capsule_weight + delta_weight\n",
    "            else:\n",
    "                interest_capsule = torch.matmul(capsule_softmax_weight, interest_item_embeddings)\n",
    "                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)\n",
    "                scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)\n",
    "                interest_capsule = scalar_factor * interest_capsule\n",
    "        interest_capsule = self.relu_linear(interest_capsule.squeeze()) # [batch_len, num_interests, hidden_dim]\n",
    "        return interest_capsule\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.to_interest_linear.parameters()] + [para for para in self.relu_linear.parameters()]\n",
    "class MIND(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, seq_len: int, num_interests: int, routing_times: int, user_profile_feature: [tuple], item_profile_feature: [tuple], profile_feature_embedding_dim: int, dnn_layer_dims: list[int]):\n",
    "        super(MIND, self).__init__()\n",
    "        self.dnn_layer_dims, self.hidden_dim = dnn_layer_dims, hidden_dim\n",
    "        self.num_interests = num_interests\n",
    "        # 内容特征\n",
    "        self.user_profile_feature, self.item_profile_feature, self.profile_feature_embedding_dim = user_profile_feature, item_profile_feature, profile_feature_embedding_dim\n",
    "        self.user_profile_embed = nn.ModuleDict({'user_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in user_profile_feature})\n",
    "        self.item_profile_embed = nn.ModuleDict({'item_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in item_profile_feature})\n",
    "        self.user_profile_all_embed_dim = profile_feature_embedding_dim * len(user_profile_feature)\n",
    "        self.item_profile_all_embed_dim = profile_feature_embedding_dim * len(item_profile_feature)\n",
    "        # item pooling layer\n",
    "        self.pooling_layer = nn.Sequential(nn.Linear(self.item_profile_all_embed_dim, hidden_dim), nn.ReLU())\n",
    "        # capsule_net\n",
    "        self.capsule_net = CapsuleNet(hidden_dim, seq_len, num_interests, routing_times)\n",
    "        # final dnn\n",
    "        self.all_embedding_dim = self.user_profile_all_embed_dim + hidden_dim\n",
    "        self.final_dnn_network = nn.Sequential(nn.Linear(self.all_embedding_dim, dnn_layer_dims[0]), nn.ReLU())\n",
    "        if len(dnn_layer_dims) > 1:\n",
    "            for i, layer_dim in enumerate(dnn_layer_dims[1:]):\n",
    "                self.final_dnn_network.append(nn.Linear(dnn_layer_dims[i], layer_dim))\n",
    "                self.final_dnn_network.append(nn.ReLU())\n",
    "        self.final_dnn_network.append(nn.Linear(dnn_layer_dims[-1], hidden_dim))\n",
    "        self.final_dnn_network.append(nn.ReLU())\n",
    "    def forward(self, user_profiles, item_history_list_profile, item_future_list_profile):\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # user profile: [batch, feature * embed_dim]\n",
    "        user_profile_embeddings = torch.cat([self.user_profile_embed['user_embed_' + str(i)](user_profiles[:,i].long()) for i in range(user_profiles.shape[-1])], axis=-1)\n",
    "        user_profile_embeddings = user_profile_embeddings.reshape((batch_len, len(self.user_profile_feature) * self.profile_feature_embedding_dim)) # [batch, feature, embed_dim]\n",
    "        # item_history_list_profile: torch.Tensor([batch, seq_len, feature * embed_dim])\n",
    "        seq_len = item_history_list_profile.shape[1]\n",
    "        item_history_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_history_list_profile[:,:,i].long()) for i in range(item_history_list_profile.shape[-1])], axis=-1)\n",
    "        item_history_list_profile_embeddings = item_history_list_profile_embeddings.reshape((batch_len, seq_len, len(self.item_profile_feature) * self.profile_feature_embedding_dim)) # [batch, seq_len, feature * embed_dim]\n",
    "        # 以上处理user profile和行为历史，下面进行与candidate组合预测， item_future_list 和 item_future_list_profile\n",
    "        seq_len_ = item_future_list_profile.shape[1]\n",
    "        item_future_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_future_list_profile[:,:,i].long()) for i in range(item_future_list_profile.shape[-1])], axis=-1)\n",
    "        item_future_list_profile_embeddings = item_future_list_profile_embeddings.reshape((batch_len, seq_len_, len(self.item_profile_feature) * self.profile_feature_embedding_dim)) # [batch, seq_len_, feature * embed_dim]\n",
    "        # [batch_len, test_len, hidden_dim]\n",
    "        # [batch, seq_len, embed_dim]\n",
    "        item_history_pool = self.pooling_layer(item_history_list_profile_embeddings)\n",
    "        mask = torch.ones((batch_len, seq_len)) # 目前我的数据集整齐没有mask\n",
    "        multi_interest_capsule = self.capsule_net(item_history_pool, mask) # [batch_len, num_interests, hidden_dim]\n",
    "        user_multi_interest_cat = torch.cat([user_profile_embeddings.unsqueeze(1).repeat((1,self.num_interests,1)), multi_interest_capsule], dim=-1)\n",
    "        user_history_multi_interest_embed = self.final_dnn_network(user_multi_interest_cat) # [batch_len, num_interests, hidden_dim]\n",
    "        # future prediction\n",
    "        item_future_pool = self.pooling_layer(item_future_list_profile_embeddings)# [batch, seq_len_, hidden_dim]\n",
    "        item_future_multi_interest_scores = torch.sigmoid(torch.bmm(item_future_pool, user_history_multi_interest_embed.permute((0,2,1)))) # [batch_len, seq_len_, num_interests]\n",
    "        # find the best capsule\n",
    "        best_interest_index = torch.argmax(item_future_multi_interest_scores, dim=-1) # [batch_len, seq_len_]\n",
    "        # print(best_interest_index)\n",
    "        best_item_future_multi_interest_score  = item_future_multi_interest_scores.take(best_interest_index).squeeze()\n",
    "        return best_item_future_multi_interest_score\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return [para for para in self.user_profile_embed.parameters()] + [para for para in self.item_profile_embed.parameters()] + [para for para in self.pooling_layer.parameters()] + [para for para in self.capsule_net.parameters()] + [para for para in self.final_dnn_network.parameters()] \n",
    "model = MIND(hidden_dim=dim, seq_len=train_seq_len, num_interests=4, routing_times=3, user_profile_feature = [(i,len(list_)) for i, list_ in user_feature_vals.items()], item_profile_feature= [(i,len(list_)) for i, list_ in item_feature_vals.items()], profile_feature_embedding_dim=dim, dnn_layer_dims=[16])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.0003)\n",
    "criterion = CrossEntropyLoss(reduction='sum').to(device)\n",
    "label = torch.FloatTensor([1 for i in range(pos_num)] + [0 for i in range(neg_sample_num)]).to(device)\n",
    "\n",
    "def DCG(batch_labels):\n",
    "    dcgsum = np.zeros((batch_labels.shape[0]))\n",
    "    for i in range(batch_labels.shape[-1]):\n",
    "        dcg = (2 ** batch_labels[:,i] - 1) / np.math.log(i + 2, 2)\n",
    "        dcgsum += dcg\n",
    "    return dcgsum\n",
    "def NDCG(output, labels):\n",
    "    # ideal_dcg\n",
    "    ideal_dcg = DCG(labels)\n",
    "    # this\n",
    "    dcg = DCG((np.argsort( - output, axis=-1)<pos_num).astype(np.float32))\n",
    "    return np.sum(dcg/ideal_dcg)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # print(item_history_list_profile.shape, item_future_list_profile.shape)\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_test_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_test_losses])\n",
    "    train_ndcg = sum([x[2] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_ndcg  = sum([x[2] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_ce_loss: {:.4f}, train_ndcg: {:.4f}, validate_ce_loss: {:.4f}, validate_ndcg: {:.4f}'.format(epoch+1, num_epochs,  train_loss, train_ndcg, test_loss, test_ndcg))\n",
    "\n",
    "# hard路由方式其实效果不佳，目前采用较为简单的网络结构。用的是item profile 没有去学item id的embedding。实际上，item id的embedding很有用。尊重原文。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:40:20.693499700Z",
     "start_time": "2023-09-05T03:39:27.433580100Z"
    }
   },
   "id": "228c1410e6c0dad4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-05 14:25:55] epoch=[1/10], train_ce_loss: 0.7489, train_ndcg: 0.6628, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:00] epoch=[2/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:05] epoch=[3/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:09] epoch=[4/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:14] epoch=[5/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:18] epoch=[6/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6676\n",
      "[2023-09-05 14:26:23] epoch=[7/10], train_ce_loss: 0.7489, train_ndcg: 0.6560, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:28] epoch=[8/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6681\n",
      "[2023-09-05 14:26:33] epoch=[9/10], train_ce_loss: 0.7489, train_ndcg: 0.6681, validate_ce_loss: 0.7489, validate_ndcg: 0.6600\n",
      "[2023-09-05 14:26:37] epoch=[10/10], train_ce_loss: 0.7489, train_ndcg: 0.6578, validate_ce_loss: 0.7492, validate_ndcg: 0.6716\n"
     ]
    }
   ],
   "source": [
    "# ComirecSA：\n",
    "# Comirec：Controllable Multi-Interest Framework for Recommendation， KDD 2020\n",
    "# 改进了MIND中的动态路由算法，采用注意力机制，并选择交互得分最大的兴趣点。\n",
    "# hard的路由模式。\n",
    "# 数据集：ml-100k\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, CrossEntropyLoss, Sequential, Linear, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else ('mps:0' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "dim=50\n",
    "\n",
    "user_feature_vals = {}\n",
    "for i in range(user_num_features):\n",
    "    user_feature_vals[i] = sorted(list(set([val[i] for val in user_info.values()])))\n",
    "    for user, info in user_info.items():\n",
    "        user_info[user][i] = user_feature_vals[i].index(info[i])\n",
    "item_feature_vals = {}\n",
    "for i in range(item_num_features):\n",
    "    item_feature_vals[i] = sorted(list(set([val[i] for val in item_info.values()])))\n",
    "    for item, info in item_info.items():\n",
    "        item_info[item][i] = item_feature_vals[i].index(info[i])\n",
    "\n",
    "user_profile_data = np.array([user_info[u] for u in data[:,0]]) # [data_len, ufeature]\n",
    "item_seq_profile_data = np.array([[item_info[item] for item in item_seq] for item_seq in data[:,1:]]) # [data_len, seq_len, ufeature]\n",
    "\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len,:]).long(),\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,train_seq_len:(train_seq_len + pos_num + neg_sample_num),:]).long()\n",
    "                                                ), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(torch.from_numpy(user_profile_data).long(), \n",
    "                                                torch.from_numpy(item_seq_profile_data[:,:train_seq_len,:]).long(), # 这里图简便，懒得改seq_len了\n",
    "                                                torch.from_numpy(item_seq_profile_data[:,-(pos_num + neg_sample_num):,:]).long()\n",
    "                                               ), batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 输入序列嵌入，得到多兴趣嵌入\n",
    "class MultiInterestSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_interests: int):\n",
    "        super(MultiInterestSelfAttention, self).__init__()\n",
    "        self.hidden_dim, self.num_interests = hidden_dim, num_interests\n",
    "        # Create trainable parameters\n",
    "        self.W1 = nn.Parameter(torch.rand(hidden_dim, hidden_dim * 4), requires_grad=True)\n",
    "        self.W2 = nn.Parameter(torch.rand(hidden_dim * 4, num_interests), requires_grad=True)\n",
    "    def forward(self, item_seq_embeds: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        H = torch.einsum('bse, ed -> bsd', item_seq_embeds, self.W1).tanh() # [batch_len, seq_len, hidden_dim * 4]\n",
    "        attention = torch.softmax(torch.einsum('bsd, dk -> bsk', H, self.W2) + -1.e9 * (1 - mask.unsqueeze(-1).float()), dim=1) # [batch_len, seq_len, num_interests]\n",
    "        multi_interest_emb = torch.matmul(attention.permute(0, 2, 1), item_seq_embeds) # [batch_len, num_interests, hidden_dim]\n",
    "        return multi_interest_emb\n",
    "class ComirecSA(nn.Module):\n",
    "    def __init__(self, hidden_dim, seq_len, num_interests, user_profile_feature: [tuple], item_profile_feature: [tuple], profile_feature_embedding_dim: int, dnn_layer_dims: list[int]):\n",
    "        super(ComirecSA, self).__init__()\n",
    "        self.hidden_dim, self.seq_len, self.num_interests = hidden_dim, seq_len, num_interests\n",
    "        # 内容特征\n",
    "        self.user_profile_feature, self.item_profile_feature, self.profile_feature_embedding_dim = user_profile_feature, item_profile_feature, profile_feature_embedding_dim\n",
    "        self.user_profile_embed = nn.ModuleDict({'user_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in user_profile_feature})\n",
    "        self.item_profile_embed = nn.ModuleDict({'item_embed_' + str(i): nn.Embedding(num_embeddings=valcount, embedding_dim=profile_feature_embedding_dim) for i, valcount in item_profile_feature})\n",
    "        self.user_profile_all_embed_dim = profile_feature_embedding_dim * len(user_profile_feature)\n",
    "        self.item_profile_all_embed_dim = profile_feature_embedding_dim * len(item_profile_feature)\n",
    "        # user/item pooling layer\n",
    "        self.user_pooling_layer = nn.Sequential(nn.Linear(self.user_profile_all_embed_dim, hidden_dim), nn.ReLU())\n",
    "        self.item_pooling_layer = nn.Sequential(nn.Linear(self.item_profile_all_embed_dim, hidden_dim), nn.ReLU())\n",
    "        # MultiInterestSelfAttention\n",
    "        self.multi_interest_sa = MultiInterestSelfAttention(hidden_dim=hidden_dim, num_interests=num_interests)\n",
    "    def forward(self, user_profiles, item_history_list_profile, item_future_list_profile):\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # user profile: [batch, feature * embed_dim]\n",
    "        user_profile_embeddings = torch.cat([self.user_profile_embed['user_embed_' + str(i)](user_profiles[:,i].long()) for i in range(user_profiles.shape[-1])], axis=-1)\n",
    "        user_profile_embeddings = self.user_pooling_layer(user_profile_embeddings.reshape((batch_len, len(self.user_profile_feature) * self.profile_feature_embedding_dim))) # [batch, embed_dim]\n",
    "        seq_len = item_history_list_profile.shape[1]\n",
    "        item_history_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_history_list_profile[:,:,i].long()) for i in range(item_history_list_profile.shape[-1])], axis=-1)\n",
    "        item_history_list_profile_embeddings = self.item_pooling_layer(item_history_list_profile_embeddings.reshape((batch_len, seq_len, len(self.item_profile_feature) * self.profile_feature_embedding_dim))) # [batch, seq_len, embed_dim]\n",
    "        # 以上处理user profile和行为历史，下面进行与candidate组合预测， item_future_list 和 item_future_list_profile\n",
    "        seq_len_ = item_future_list_profile.shape[1]\n",
    "        item_future_list_profile_embeddings = torch.cat([self.item_profile_embed['item_embed_' + str(i)](item_future_list_profile[:,:,i].long()) for i in range(item_future_list_profile.shape[-1])], axis=-1)\n",
    "        item_future_list_profile_embeddings = self.item_pooling_layer(item_future_list_profile_embeddings.reshape((batch_len, seq_len_, len(self.item_profile_feature) * self.profile_feature_embedding_dim))) # [batch, seq_len_, embed_dim]\n",
    "        mask = torch.ones((batch_len, seq_len)) # 目前我的数据集整齐没有mask\n",
    "        \n",
    "        # [batch_len, seq_len, embed_dim] + [batch, seq_len, embed_dim] ->  [batch_len, num_interests, hidden_dim]\n",
    "        multi_interest_embeds = self.multi_interest_sa(user_profile_embeddings.unsqueeze(1) * item_history_list_profile_embeddings, mask)\n",
    "        # [batch, seq_len_, hidden_dim], [batch_len, num_interests, hidden_dim] -> [batch, seq_len_, num_interests]\n",
    "        future_interest_scores = torch.sigmoid(torch.bmm(item_future_list_profile_embeddings, multi_interest_embeds.permute((0,2,1))))\n",
    "        best_future_index = torch.argmax(future_interest_scores, dim=-1)\n",
    "        best_item_future_multi_interest_score  = future_interest_scores.take(best_future_index).squeeze()\n",
    "        return best_item_future_multi_interest_score\n",
    "model = ComirecSA(hidden_dim=dim, seq_len=train_seq_len, num_interests=4, user_profile_feature = [(i,len(list_)) for i, list_ in user_feature_vals.items()], item_profile_feature= [(i,len(list_)) for i, list_ in item_feature_vals.items()], profile_feature_embedding_dim=dim, dnn_layer_dims=[16])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.0003)\n",
    "criterion = CrossEntropyLoss(reduction='sum').to(device)\n",
    "label = torch.FloatTensor([1 for i in range(pos_num)] + [0 for i in range(neg_sample_num)]).to(device)\n",
    "\n",
    "def DCG(batch_labels):\n",
    "    dcgsum = np.zeros((batch_labels.shape[0]))\n",
    "    for i in range(batch_labels.shape[-1]):\n",
    "        dcg = (2 ** batch_labels[:,i] - 1) / np.math.log(i + 2, 2)\n",
    "        dcgsum += dcg\n",
    "    return dcgsum\n",
    "def NDCG(output, labels):\n",
    "    # ideal_dcg\n",
    "    ideal_dcg = DCG(labels)\n",
    "    # this\n",
    "    dcg = DCG((np.argsort( - output, axis=-1)<pos_num).astype(np.float32))\n",
    "    return np.sum(dcg/ideal_dcg)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train:\n",
    "    epoch_train_losses = []\n",
    "    model.train()\n",
    "    for i, inputs in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        # print(item_history_list_profile.shape, item_future_list_profile.shape)\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    for i, inputs in enumerate(test_loader):\n",
    "        user_profiles, item_history_list_profile, item_future_list_profile = inputs\n",
    "        batch_len = user_profiles.shape[0]\n",
    "        user_profiles = user_profiles.to(device)\n",
    "        item_history_list_profile = item_history_list_profile.to(device)\n",
    "        item_future_list_profile = item_future_list_profile.to(device)\n",
    "        output = model(user_profiles, item_history_list_profile, item_future_list_profile)\n",
    "        labels = label.unsqueeze(0).repeat([batch_len,1])\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_test_losses.append([batch_len, loss.item(), NDCG(output.cpu().detach().numpy(), labels.cpu().detach().numpy())])\n",
    "    train_loss = sum([x[1] for x in epoch_train_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_train_losses])\n",
    "    test_loss  = sum([x[1] for x in epoch_test_losses])/sum([x[0] * (pos_num + neg_sample_num) for x in epoch_test_losses])\n",
    "    train_ndcg = sum([x[2] for x in epoch_train_losses])/sum([x[0] for x in epoch_train_losses])\n",
    "    test_ndcg  = sum([x[2] for x in epoch_test_losses])/sum([x[0] for x in epoch_test_losses])\n",
    "    # print\n",
    "    print('['+datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")+']', 'epoch=[{}/{}], train_ce_loss: {:.4f}, train_ndcg: {:.4f}, validate_ce_loss: {:.4f}, validate_ndcg: {:.4f}'.format(epoch+1, num_epochs,  train_loss, train_ndcg, test_loss, test_ndcg))\n",
    "\n",
    "\n",
    "# hard路由方式其实效果不佳，目前采用较为简单的网络结构。用的是item profile 没有去学item id的embedding。实际上，item id的embedding很有用。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T06:26:37.763860Z",
     "start_time": "2023-09-05T06:25:50.958627500Z"
    }
   },
   "id": "da4e73563524efc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "889bc22a48444987"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
